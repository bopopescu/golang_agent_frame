diff --git b/flux/stdlib/influxdata/influxdb/buckets.go a/flux/stdlib/influxdata/influxdb/buckets.go
index 4fd36f948..9ecbe4923 100644
--- b/flux/stdlib/influxdata/influxdb/buckets.go
+++ a/flux/stdlib/influxdata/influxdb/buckets.go
@@ -2,6 +2,7 @@ package influxdb
 
 import (
 	"context"
+	"errors"
 	"fmt"
 
 	"github.com/influxdata/flux"
@@ -11,8 +12,8 @@ import (
 	"github.com/influxdata/flux/plan"
 	"github.com/influxdata/flux/stdlib/influxdata/influxdb"
 	"github.com/influxdata/flux/values"
-	platform "github.com/influxdata/influxdb"
-	"github.com/influxdata/influxdb/query"
+	"github.com/influxdata/influxdb/services/meta"
+	"github.com/influxdata/influxql"
 )
 
 func init() {
@@ -20,10 +21,9 @@ func init() {
 }
 
 type BucketsDecoder struct {
-	orgID   platform.ID
-	deps    BucketDependencies
-	buckets []*platform.Bucket
-	alloc   *memory.Allocator
+	deps  StorageDependencies
+	alloc *memory.Allocator
+	user  meta.User
 }
 
 func (bd *BucketsDecoder) Connect(ctx context.Context) error {
@@ -31,20 +31,12 @@ func (bd *BucketsDecoder) Connect(ctx context.Context) error {
 }
 
 func (bd *BucketsDecoder) Fetch(ctx context.Context) (bool, error) {
-	b, count := bd.deps.FindAllBuckets(ctx, bd.orgID)
-	if count <= 0 {
-		return false, &flux.Error{
-			Code: codes.NotFound,
-			Msg:  fmt.Sprintf("no buckets found in organization %v", bd.orgID),
-		}
-	}
-	bd.buckets = b
 	return false, nil
 }
 
 func (bd *BucketsDecoder) Decode(ctx context.Context) (flux.Table, error) {
 	kb := execute.NewGroupKeyBuilder(nil)
-	kb.AddKeyValue("organizationID", values.NewString(bd.buckets[0].OrgID.String()))
+	kb.AddKeyValue("organizationID", values.NewString(""))
 	gk, err := kb.Build()
 	if err != nil {
 		return nil, err
@@ -52,43 +44,50 @@ func (bd *BucketsDecoder) Decode(ctx context.Context) (flux.Table, error) {
 
 	b := execute.NewColListTableBuilder(gk, bd.alloc)
 
-	if _, err := b.AddCol(flux.ColMeta{
+	_, _ = b.AddCol(flux.ColMeta{
 		Label: "name",
 		Type:  flux.TString,
-	}); err != nil {
-		return nil, err
-	}
-	if _, err := b.AddCol(flux.ColMeta{
+	})
+	_, _ = b.AddCol(flux.ColMeta{
 		Label: "id",
 		Type:  flux.TString,
-	}); err != nil {
-		return nil, err
-	}
-	if _, err := b.AddCol(flux.ColMeta{
+	})
+	_, _ = b.AddCol(flux.ColMeta{
 		Label: "organizationID",
 		Type:  flux.TString,
-	}); err != nil {
-		return nil, err
-	}
-	if _, err := b.AddCol(flux.ColMeta{
+	})
+	_, _ = b.AddCol(flux.ColMeta{
 		Label: "retentionPolicy",
 		Type:  flux.TString,
-	}); err != nil {
-		return nil, err
-	}
-	if _, err := b.AddCol(flux.ColMeta{
+	})
+	_, _ = b.AddCol(flux.ColMeta{
 		Label: "retentionPeriod",
 		Type:  flux.TInt,
-	}); err != nil {
-		return nil, err
+	})
+
+	var hasAccess func(db string) bool
+	if bd.user == nil {
+		hasAccess = func(db string) bool {
+			return true
+		}
+	} else {
+		hasAccess = func(db string) bool {
+			return bd.deps.Authorizer.AuthorizeDatabase(bd.user, influxql.ReadPrivilege, db) == nil ||
+				bd.deps.Authorizer.AuthorizeDatabase(bd.user, influxql.WritePrivilege, db) == nil
+		}
 	}
 
-	for _, bucket := range bd.buckets {
-		_ = b.AppendString(0, bucket.Name)
-		_ = b.AppendString(1, bucket.ID.String())
-		_ = b.AppendString(2, bucket.OrgID.String())
-		_ = b.AppendString(3, bucket.RetentionPolicyName)
-		_ = b.AppendInt(4, bucket.RetentionPeriod.Nanoseconds())
+	for _, db := range bd.deps.MetaClient.Databases() {
+		if hasAccess(db.Name) {
+			for _, rp := range db.RetentionPolicies {
+				_ = b.AppendString(0, db.Name+"/"+rp.Name)
+				_ = b.AppendString(1, "")
+				_ = b.AppendString(2, "influxdb")
+				_ = b.AppendString(3, "")
+				_ = b.AppendString(4, rp.Name)
+				_ = b.AppendInt(5, rp.Duration.Nanoseconds())
+			}
+		}
 	}
 
 	return b.Table()
@@ -109,22 +108,22 @@ func createBucketsSource(prSpec plan.ProcedureSpec, dsid execute.DatasetID, a ex
 
 	// the dependencies used for FromKind are adequate for what we need here
 	// so there's no need to inject custom dependencies for buckets()
-	deps := GetStorageDependencies(a.Context()).BucketDeps
-	req := query.RequestFromContext(a.Context())
-	if req == nil {
-		return nil, &flux.Error{
-			Code: codes.Internal,
-			Msg:  "missing request on context",
+	deps := GetStorageDependencies(a.Context())
+
+	var user meta.User
+	if deps.AuthEnabled {
+		user = meta.UserFromContext(a.Context())
+		if user == nil {
+			return nil, errors.New("createBucketsSource: no user")
 		}
 	}
-	orgID := req.OrganizationID
 
-	bd := &BucketsDecoder{orgID: orgID, deps: deps, alloc: a.Allocator()}
+	bd := &BucketsDecoder{deps: deps, alloc: a.Allocator(), user: user}
 
 	return execute.CreateSourceFromDecoder(bd, dsid, a)
 }
 
-type AllBucketLookup interface {
-	FindAllBuckets(ctx context.Context, orgID platform.ID) ([]*platform.Bucket, int)
+type MetaClient interface {
+	Databases() []meta.DatabaseInfo
+	Database(name string) *meta.DatabaseInfo
 }
-type BucketDependencies AllBucketLookup
diff --git b/flux/stdlib/influxdata/influxdb/dependencies.go a/flux/stdlib/influxdata/influxdb/dependencies.go
index 3303c2758..ad9a36ab6 100644
--- b/flux/stdlib/influxdata/influxdb/dependencies.go
+++ a/flux/stdlib/influxdata/influxdb/dependencies.go
@@ -2,13 +2,9 @@ package influxdb
 
 import (
 	"context"
+	"errors"
 
 	"github.com/influxdata/flux"
-	"github.com/influxdata/influxdb"
-	"github.com/influxdata/influxdb/kit/prom"
-	"github.com/influxdata/influxdb/query"
-	"github.com/influxdata/influxdb/storage"
-	"github.com/prometheus/client_golang/prometheus"
 )
 
 type key int
@@ -16,33 +12,31 @@ type key int
 const dependenciesKey key = iota
 
 type StorageDependencies struct {
-	FromDeps   FromDependencies
-	BucketDeps BucketDependencies
-	ToDeps     ToDependencies
+	Reader      Reader
+	MetaClient  MetaClient
+	Authorizer  Authorizer
+	AuthEnabled bool
 }
 
 func (d StorageDependencies) Inject(ctx context.Context) context.Context {
 	return context.WithValue(ctx, dependenciesKey, d)
 }
 
-func GetStorageDependencies(ctx context.Context) StorageDependencies {
-	return ctx.Value(dependenciesKey).(StorageDependencies)
-}
-
-// PrometheusCollectors satisfies the prom.PrometheusCollector interface.
-func (d StorageDependencies) PrometheusCollectors() []prometheus.Collector {
-	depS := []interface{}{
-		d.FromDeps,
-		d.BucketDeps,
-		d.ToDeps,
+func (d StorageDependencies) Validate() error {
+	if d.Reader == nil {
+		return errors.New("missing reader dependency")
 	}
-	collectors := make([]prometheus.Collector, 0, len(depS))
-	for _, v := range depS {
-		if pc, ok := v.(prom.PrometheusCollector); ok {
-			collectors = append(collectors, pc.PrometheusCollectors()...)
-		}
+	if d.MetaClient == nil {
+		return errors.New("missing meta client dependency")
 	}
-	return collectors
+	if d.AuthEnabled && d.Authorizer == nil {
+		return errors.New("missing authorizer dependency")
+	}
+	return nil
+}
+
+func GetStorageDependencies(ctx context.Context) StorageDependencies {
+	return ctx.Value(dependenciesKey).(StorageDependencies)
 }
 
 type Dependencies struct {
@@ -55,45 +49,21 @@ func (d Dependencies) Inject(ctx context.Context) context.Context {
 	return d.StorageDeps.Inject(ctx)
 }
 
-// PrometheusCollectors satisfies the prom.PrometheusCollector interface.
-func (d Dependencies) PrometheusCollectors() []prometheus.Collector {
-	collectors := d.StorageDeps.PrometheusCollectors()
-	if pc, ok := d.FluxDeps.(prom.PrometheusCollector); ok {
-		collectors = append(collectors, pc.PrometheusCollectors()...)
-	}
-	return collectors
-}
-
 func NewDependencies(
+	mc MetaClient,
 	reader Reader,
-	writer storage.PointsWriter,
-	bucketSvc influxdb.BucketService,
-	orgSvc influxdb.OrganizationService,
-	ss influxdb.SecretService,
-	metricLabelKeys []string,
+	auth Authorizer,
+	authEnabled bool,
 ) (Dependencies, error) {
 	fdeps := flux.NewDefaultDependencies()
-	fdeps.Deps.SecretService = query.FromSecretService(ss)
 	deps := Dependencies{FluxDeps: fdeps}
-	bucketLookupSvc := query.FromBucketService(bucketSvc)
-	orgLookupSvc := query.FromOrganizationService(orgSvc)
-	metrics := NewMetrics(metricLabelKeys)
-	deps.StorageDeps.FromDeps = FromDependencies{
-		Reader:             reader,
-		BucketLookup:       bucketLookupSvc,
-		OrganizationLookup: orgLookupSvc,
-		Metrics:            metrics,
-	}
-	if err := deps.StorageDeps.FromDeps.Validate(); err != nil {
-		return Dependencies{}, err
-	}
-	deps.StorageDeps.BucketDeps = bucketLookupSvc
-	deps.StorageDeps.ToDeps = ToDependencies{
-		BucketLookup:       bucketLookupSvc,
-		OrganizationLookup: orgLookupSvc,
-		PointsWriter:       writer,
+	deps.StorageDeps = StorageDependencies{
+		Reader:      reader,
+		MetaClient:  mc,
+		Authorizer:  auth,
+		AuthEnabled: authEnabled,
 	}
-	if err := deps.StorageDeps.ToDeps.Validate(); err != nil {
+	if err := deps.StorageDeps.Validate(); err != nil {
 		return Dependencies{}, err
 	}
 	return deps, nil
diff --git b/flux/stdlib/influxdata/influxdb/from.go a/flux/stdlib/influxdata/influxdb/from.go
index cdd6789c1..6662f54fd 100644
--- b/flux/stdlib/influxdata/influxdb/from.go
+++ a/flux/stdlib/influxdata/influxdb/from.go
@@ -8,7 +8,6 @@ import (
 	"github.com/influxdata/flux/plan"
 	"github.com/influxdata/flux/semantic"
 	"github.com/influxdata/flux/stdlib/influxdata/influxdb"
-	platform "github.com/influxdata/influxdb"
 )
 
 const FromKind = "influxDBFrom"
@@ -71,31 +70,6 @@ func (s *FromOpSpec) Kind() flux.OperationKind {
 	return FromKind
 }
 
-// BucketsAccessed makes FromOpSpec a query.BucketAwareOperationSpec
-func (s *FromOpSpec) BucketsAccessed(orgID *platform.ID) (readBuckets, writeBuckets []platform.BucketFilter) {
-	bf := platform.BucketFilter{}
-	if s.Bucket != "" {
-		bf.Name = &s.Bucket
-	}
-	if orgID != nil {
-		bf.OrganizationID = orgID
-	}
-
-	if len(s.BucketID) > 0 {
-		if id, err := platform.IDFromString(s.BucketID); err != nil {
-			invalidID := platform.InvalidID()
-			bf.ID = &invalidID
-		} else {
-			bf.ID = id
-		}
-	}
-
-	if bf.ID != nil || bf.Name != nil {
-		readBuckets = append(readBuckets, bf)
-	}
-	return readBuckets, writeBuckets
-}
-
 type FromProcedureSpec struct {
 	Bucket   string
 	BucketID string
diff --git b/flux/stdlib/influxdata/influxdb/from_test.go a/flux/stdlib/influxdata/influxdb/from_test.go
index dac3b13ee..daba8c936 100644
--- b/flux/stdlib/influxdata/influxdb/from_test.go
+++ a/flux/stdlib/influxdata/influxdb/from_test.go
@@ -1,192 +1 @@
 package influxdb_test
-
-import (
-	"fmt"
-	"testing"
-	"time"
-
-	"github.com/influxdata/flux"
-	"github.com/influxdata/flux/execute"
-	"github.com/influxdata/flux/plan"
-	"github.com/influxdata/flux/plan/plantest"
-	"github.com/influxdata/flux/querytest"
-	"github.com/influxdata/flux/stdlib/universe"
-	platform "github.com/influxdata/influxdb"
-	pquerytest "github.com/influxdata/influxdb/query/querytest"
-	"github.com/influxdata/influxdb/query/stdlib/influxdata/influxdb"
-)
-
-func TestFrom_NewQuery(t *testing.T) {
-	t.Skip()
-	tests := []querytest.NewQueryTestCase{
-		{
-			Name:    "from no args",
-			Raw:     `from()`,
-			WantErr: true,
-		},
-		{
-			Name:    "from conflicting args",
-			Raw:     `from(bucket:"d", bucket:"b")`,
-			WantErr: true,
-		},
-		{
-			Name:    "from repeat arg",
-			Raw:     `from(bucket:"telegraf", bucket:"oops")`,
-			WantErr: true,
-		},
-		{
-			Name:    "from",
-			Raw:     `from(bucket:"telegraf", chicken:"what is this?")`,
-			WantErr: true,
-		},
-		{
-			Name:    "from bucket invalid ID",
-			Raw:     `from(bucketID:"invalid")`,
-			WantErr: true,
-		},
-		{
-			Name: "from bucket ID",
-			Raw:  `from(bucketID:"aaaabbbbccccdddd")`,
-			Want: &flux.Spec{
-				Operations: []*flux.Operation{
-					{
-						ID: "from0",
-						Spec: &influxdb.FromOpSpec{
-							BucketID: "aaaabbbbccccdddd",
-						},
-					},
-				},
-			},
-		},
-		{
-			Name: "from with database",
-			Raw:  `from(bucket:"mybucket") |> range(start:-4h, stop:-2h) |> sum()`,
-			Want: &flux.Spec{
-				Operations: []*flux.Operation{
-					{
-						ID: "from0",
-						Spec: &influxdb.FromOpSpec{
-							Bucket: "mybucket",
-						},
-					},
-					{
-						ID: "range1",
-						Spec: &universe.RangeOpSpec{
-							Start: flux.Time{
-								Relative:   -4 * time.Hour,
-								IsRelative: true,
-							},
-							Stop: flux.Time{
-								Relative:   -2 * time.Hour,
-								IsRelative: true,
-							},
-							TimeColumn:  "_time",
-							StartColumn: "_start",
-							StopColumn:  "_stop",
-						},
-					},
-					{
-						ID: "sum2",
-						Spec: &universe.SumOpSpec{
-							AggregateConfig: execute.DefaultAggregateConfig,
-						},
-					},
-				},
-				Edges: []flux.Edge{
-					{Parent: "from0", Child: "range1"},
-					{Parent: "range1", Child: "sum2"},
-				},
-			},
-		},
-	}
-	for _, tc := range tests {
-		tc := tc
-		t.Run(tc.Name, func(t *testing.T) {
-			t.Parallel()
-			querytest.NewQueryTestHelper(t, tc)
-		})
-	}
-}
-
-func TestFromOperation_Marshaling(t *testing.T) {
-	t.Skip()
-	data := []byte(`{"id":"from","kind":"from","spec":{"bucket":"mybucket"}}`)
-	op := &flux.Operation{
-		ID: "from",
-		Spec: &influxdb.FromOpSpec{
-			Bucket: "mybucket",
-		},
-	}
-	querytest.OperationMarshalingTestHelper(t, data, op)
-}
-
-func TestFromOpSpec_BucketsAccessed(t *testing.T) {
-	bucketName := "my_bucket"
-	bucketIDString := "aaaabbbbccccdddd"
-	bucketID, err := platform.IDFromString(bucketIDString)
-	if err != nil {
-		t.Fatal(err)
-	}
-	invalidID := platform.InvalidID()
-	tests := []pquerytest.BucketsAccessedTestCase{
-		{
-			Name:             "From with bucket",
-			Raw:              fmt.Sprintf(`from(bucket:"%s")`, bucketName),
-			WantReadBuckets:  &[]platform.BucketFilter{{Name: &bucketName}},
-			WantWriteBuckets: &[]platform.BucketFilter{},
-		},
-		{
-			Name:             "From with bucketID",
-			Raw:              fmt.Sprintf(`from(bucketID:"%s")`, bucketID),
-			WantReadBuckets:  &[]platform.BucketFilter{{ID: bucketID}},
-			WantWriteBuckets: &[]platform.BucketFilter{},
-		},
-		{
-			Name:             "From invalid bucketID",
-			Raw:              `from(bucketID:"invalid")`,
-			WantReadBuckets:  &[]platform.BucketFilter{{ID: &invalidID}},
-			WantWriteBuckets: &[]platform.BucketFilter{},
-		},
-	}
-	for _, tc := range tests {
-		tc := tc
-		t.Run(tc.Name, func(t *testing.T) {
-			t.Parallel()
-			pquerytest.BucketsAccessedTestHelper(t, tc)
-		})
-	}
-}
-
-func TestFromValidation(t *testing.T) {
-	spec := plantest.PlanSpec{
-		// from |> group (cannot query an infinite time range)
-		Nodes: []plan.Node{
-			plan.CreateLogicalNode("from", &influxdb.FromProcedureSpec{
-				Bucket: "my-bucket",
-			}),
-			plan.CreatePhysicalNode("group", &universe.GroupProcedureSpec{
-				GroupMode: flux.GroupModeBy,
-				GroupKeys: []string{"_measurement", "_field"},
-			}),
-		},
-		Edges: [][2]int{
-			{0, 1},
-		},
-	}
-
-	ps := plantest.CreatePlanSpec(&spec)
-	pp := plan.NewPhysicalPlanner(plan.OnlyPhysicalRules(
-		influxdb.PushDownRangeRule{},
-		influxdb.PushDownFilterRule{},
-		influxdb.PushDownGroupRule{},
-	))
-	_, err := pp.Plan(ps)
-	if err == nil {
-		t.Error("Expected query with no call to range to fail physical planning")
-	}
-	want := `cannot submit unbounded read to "my-bucket"; try bounding 'from' with a call to 'range'`
-	got := err.Error()
-	if want != got {
-		t.Errorf("unexpected error; -want/+got\n- %s\n+ %s", want, got)
-	}
-}
diff --git b/flux/stdlib/influxdata/influxdb/metrics.go a/flux/stdlib/influxdata/influxdb/metrics.go
index 82577e3a5..dd3cee868 100644
--- b/flux/stdlib/influxdata/influxdb/metrics.go
+++ a/flux/stdlib/influxdata/influxdb/metrics.go
@@ -1,83 +1,3 @@
 package influxdb
 
-import (
-	"context"
-	"fmt"
-	"time"
-
-	platform "github.com/influxdata/influxdb"
-	"github.com/prometheus/client_golang/prometheus"
-)
-
-const (
-	orgLabel = "org"
-	opLabel  = "op"
-)
-
-type metrics struct {
-	ctxLabelKeys []string
-	requestDur   *prometheus.HistogramVec
-}
-
-// NewMetrics produces a new metrics objects for an influxdb source.
-// Currently it just collects the duration of read requests into a histogram.
-// ctxLabelKeys is a list of labels to add to the produced metrics.
-// The value for a given key will be read off the context.
-// The context value must be a string or an implementation of the Stringer interface.
-// In addition, produced metrics will be labeled with the orgID and type of operation requested.
-func NewMetrics(ctxLabelKeys []string) *metrics {
-	labelKeys := make([]string, len(ctxLabelKeys)+2)
-	copy(labelKeys, ctxLabelKeys)
-	labelKeys[len(labelKeys)-2] = orgLabel
-	labelKeys[len(labelKeys)-1] = opLabel
-
-	m := new(metrics)
-	m.requestDur = prometheus.NewHistogramVec(prometheus.HistogramOpts{
-		Namespace: "query",
-		Subsystem: "influxdb_source",
-		Name:      "read_request_duration_seconds",
-		Help:      "Histogram of times spent in read requests",
-		Buckets:   prometheus.ExponentialBuckets(1e-3, 5, 7),
-	}, labelKeys)
-	m.ctxLabelKeys = ctxLabelKeys
-	return m
-}
-
-// PrometheusCollectors satisfies the PrometheusCollector interface.
-func (m *metrics) PrometheusCollectors() []prometheus.Collector {
-	if m == nil {
-		// if metrics happens to be nil here (such as for a test), then let's not panic.
-		return nil
-	}
-	return []prometheus.Collector{
-		m.requestDur,
-	}
-}
-
-func (m *metrics) getLabelValues(ctx context.Context, orgID platform.ID, op string) []string {
-	if m == nil {
-		return nil
-	}
-	labelValues := make([]string, len(m.ctxLabelKeys)+2)
-	for i, k := range m.ctxLabelKeys {
-		value := ctx.Value(k)
-		var str string
-		switch v := value.(type) {
-		case string:
-			str = v
-		case fmt.Stringer:
-			str = v.String()
-		}
-		labelValues[i] = str
-	}
-	labelValues[len(labelValues)-2] = orgID.String()
-	labelValues[len(labelValues)-1] = op
-	return labelValues
-}
-
-func (m *metrics) recordMetrics(labelValues []string, start time.Time) {
-	if m == nil {
-		return
-	}
-	m.requestDur.WithLabelValues(labelValues...).Observe(time.Since(start).Seconds())
-}
+// Storage metrics are not implemented in the 1.x flux engine.
diff --git b/flux/stdlib/influxdata/influxdb/operators.go a/flux/stdlib/influxdata/influxdb/operators.go
index 4203b074b..b0db49591 100644
--- b/flux/stdlib/influxdata/influxdb/operators.go
+++ a/flux/stdlib/influxdata/influxdb/operators.go
@@ -2,14 +2,16 @@ package influxdb
 
 import (
 	"context"
-	"fmt"
+	"errors"
+	"strings"
 
 	"github.com/influxdata/flux"
-	"github.com/influxdata/flux/codes"
+	"github.com/influxdata/flux/execute"
 	"github.com/influxdata/flux/plan"
 	"github.com/influxdata/flux/semantic"
 	"github.com/influxdata/flux/values"
-	"github.com/influxdata/influxdb"
+	"github.com/influxdata/influxdb/services/meta"
+	"github.com/influxdata/influxql"
 )
 
 const (
@@ -79,34 +81,43 @@ func (s *ReadRangePhysSpec) Copy() plan.ProcedureSpec {
 	return ns
 }
 
-func (s *ReadRangePhysSpec) LookupBucketID(ctx context.Context, orgID influxdb.ID, buckets BucketLookup) (influxdb.ID, error) {
-	// Determine bucketID
-	switch {
-	case s.Bucket != "":
-		b, ok := buckets.Lookup(ctx, orgID, s.Bucket)
-		if !ok {
-			return 0, &flux.Error{
-				Code: codes.NotFound,
-				Msg:  fmt.Sprintf("could not find bucket %q", s.Bucket),
-			}
-		}
-		return b, nil
-	case len(s.BucketID) != 0:
-		var b influxdb.ID
-		if err := b.DecodeFromString(s.BucketID); err != nil {
-			return 0, &flux.Error{
-				Code: codes.Invalid,
-				Msg:  "invalid bucket id",
-				Err:  err,
-			}
+func (s *ReadRangePhysSpec) LookupDatabase(ctx context.Context, deps StorageDependencies, a execute.Administration) (string, string, error) {
+	if len(s.BucketID) != 0 {
+		return "", "", errors.New("cannot refer to buckets by their id in 1.x")
+	}
+
+	var db, rp string
+	if i := strings.IndexByte(s.Bucket, '/'); i == -1 {
+		db = s.Bucket
+	} else {
+		rp = s.Bucket[i+1:]
+		db = s.Bucket[:i]
+	}
+
+	// validate and resolve db/rp
+	di := deps.MetaClient.Database(db)
+	if di == nil {
+		return "", "", errors.New("no database")
+	}
+
+	if deps.AuthEnabled {
+		user := meta.UserFromContext(a.Context())
+		if user == nil {
+			return "", "", errors.New("createFromSource: no user")
 		}
-		return b, nil
-	default:
-		return 0, &flux.Error{
-			Code: codes.Invalid,
-			Msg:  "no bucket name or id have been specified",
+		if err := deps.Authorizer.AuthorizeDatabase(user, influxql.ReadPrivilege, db); err != nil {
+			return "", "", err
 		}
 	}
+
+	if rp == "" {
+		rp = di.DefaultRetentionPolicy
+	}
+
+	if rpi := di.RetentionPolicy(rp); rpi == nil {
+		return "", "", errors.New("invalid retention policy")
+	}
+	return db, rp, nil
 }
 
 // TimeBounds implements plan.BoundsAwareProcedureSpec.
diff --git b/flux/stdlib/influxdata/influxdb/rules.go a/flux/stdlib/influxdata/influxdb/rules.go
index 9e9e66283..4b6425ac9 100644
--- b/flux/stdlib/influxdata/influxdb/rules.go
+++ a/flux/stdlib/influxdata/influxdb/rules.go
@@ -192,6 +192,12 @@ func (rule PushDownReadTagKeysRule) Rewrite(pn plan.Node) (plan.Node, bool, erro
 	// constructing our own replacement. We do not care about it
 	// at the moment though which is why it is not in the pattern.
 
+	// The tag keys mechanism doesn't know about fields so we cannot
+	// push down _field comparisons in 1.x.
+	if hasFieldExpr(fromSpec.Filter) {
+		return pn, false, nil
+	}
+
 	// The schema mutator needs to correspond to a keep call
 	// on the column specified by the keys procedure.
 	if len(keepSpec.Mutations) != 1 {
@@ -221,6 +227,20 @@ func (rule PushDownReadTagKeysRule) Rewrite(pn plan.Node) (plan.Node, bool, erro
 	}), true, nil
 }
 
+func hasFieldExpr(expr semantic.Expression) bool {
+	hasField := false
+	v := semantic.CreateVisitor(func(node semantic.Node) {
+		switch n := node.(type) {
+		case *semantic.MemberExpression:
+			if n.Property == "_field" {
+				hasField = true
+			}
+		}
+	})
+	semantic.Walk(v, expr)
+	return hasField
+}
+
 // PushDownReadTagValuesRule matches 'ReadRange |> keep(columns: [tag]) |> group() |> distinct(column: tag)'.
 // The 'from()' must have already been merged with 'range' and, optionally,
 // may have been merged with 'filter'.
@@ -298,6 +318,9 @@ var invalidTagKeysForTagValues = []string{
 	execute.DefaultValueColLabel,
 	execute.DefaultStartColLabel,
 	execute.DefaultStopColLabel,
+	// TODO(jsternberg): There just doesn't seem to be a good way to do this
+	// in the 1.x line of the release.
+	"_field",
 }
 
 // isValidTagKeyForTagValues returns true if the given key can
diff --git b/flux/stdlib/influxdata/influxdb/rules_test.go a/flux/stdlib/influxdata/influxdb/rules_test.go
index 12514b8d8..676deda55 100644
--- b/flux/stdlib/influxdata/influxdb/rules_test.go
+++ a/flux/stdlib/influxdata/influxdb/rules_test.go
@@ -12,7 +12,7 @@ import (
 	"github.com/influxdata/flux/plan/plantest"
 	"github.com/influxdata/flux/semantic"
 	"github.com/influxdata/flux/stdlib/universe"
-	"github.com/influxdata/influxdb/query/stdlib/influxdata/influxdb"
+	"github.com/influxdata/influxdb/flux/stdlib/influxdata/influxdb"
 )
 
 func fluxTime(t int64) flux.Time {
diff --git b/flux/stdlib/influxdata/influxdb/source.go a/flux/stdlib/influxdata/influxdb/source.go
index 1cff76b5b..3e0d5b654 100644
--- b/flux/stdlib/influxdata/influxdb/source.go
+++ a/flux/stdlib/influxdata/influxdb/source.go
@@ -3,7 +3,6 @@ package influxdb
 import (
 	"context"
 	"errors"
-	"time"
 
 	"github.com/influxdata/flux"
 	"github.com/influxdata/flux/codes"
@@ -11,9 +10,6 @@ import (
 	"github.com/influxdata/flux/memory"
 	"github.com/influxdata/flux/plan"
 	"github.com/influxdata/flux/semantic"
-	platform "github.com/influxdata/influxdb"
-	"github.com/influxdata/influxdb/kit/tracing"
-	"github.com/influxdata/influxdb/query"
 	"github.com/influxdata/influxdb/tsdb/cursors"
 )
 
@@ -36,17 +32,10 @@ type Source struct {
 	stats cursors.CursorStats
 
 	runner runner
-
-	m     *metrics
-	orgID platform.ID
-	op    string
 }
 
 func (s *Source) Run(ctx context.Context) {
-	labelValues := s.m.getLabelValues(ctx, s.orgID, s.op)
-	start := time.Now()
 	err := s.runner.run(ctx)
-	s.m.recordMetrics(labelValues, start)
 	for _, t := range s.ts {
 		t.Finish(s.id, err)
 	}
@@ -123,10 +112,6 @@ func ReadFilterSource(id execute.DatasetID, r Reader, readSpec ReadFilterSpec, a
 	src.reader = r
 	src.readSpec = readSpec
 
-	src.m = GetStorageDependencies(a.Context()).FromDeps.Metrics
-	src.orgID = readSpec.OrganizationID
-	src.op = "readFilter"
-
 	src.runner = src
 	return src
 }
@@ -145,8 +130,7 @@ func (s *readFilterSource) run(ctx context.Context) error {
 }
 
 func createReadFilterSource(s plan.ProcedureSpec, id execute.DatasetID, a execute.Administration) (execute.Source, error) {
-	span, ctx := tracing.StartSpanFromContext(a.Context())
-	defer span.Finish()
+	ctx := a.Context()
 
 	spec := s.(*ReadRangePhysSpec)
 
@@ -158,18 +142,9 @@ func createReadFilterSource(s plan.ProcedureSpec, id execute.DatasetID, a execut
 		}
 	}
 
-	deps := GetStorageDependencies(a.Context()).FromDeps
-
-	req := query.RequestFromContext(a.Context())
-	if req == nil {
-		return nil, &flux.Error{
-			Code: codes.Internal,
-			Msg:  "missing request on context",
-		}
-	}
+	deps := GetStorageDependencies(a.Context())
 
-	orgID := req.OrganizationID
-	bucketID, err := spec.LookupBucketID(ctx, orgID, deps.BucketLookup)
+	db, rp, err := spec.LookupDatabase(ctx, deps, a)
 	if err != nil {
 		return nil, err
 	}
@@ -182,10 +157,10 @@ func createReadFilterSource(s plan.ProcedureSpec, id execute.DatasetID, a execut
 		id,
 		deps.Reader,
 		ReadFilterSpec{
-			OrganizationID: orgID,
-			BucketID:       bucketID,
-			Bounds:         *bounds,
-			Predicate:      filter,
+			Database:        db,
+			RetentionPolicy: rp,
+			Bounds:          *bounds,
+			Predicate:       filter,
 		},
 		a,
 	), nil
@@ -206,10 +181,6 @@ func ReadGroupSource(id execute.DatasetID, r Reader, readSpec ReadGroupSpec, a e
 	src.reader = r
 	src.readSpec = readSpec
 
-	src.m = GetStorageDependencies(a.Context()).FromDeps.Metrics
-	src.orgID = readSpec.OrganizationID
-	src.op = "readGroup"
-
 	src.runner = src
 	return src
 }
@@ -228,8 +199,7 @@ func (s *readGroupSource) run(ctx context.Context) error {
 }
 
 func createReadGroupSource(s plan.ProcedureSpec, id execute.DatasetID, a execute.Administration) (execute.Source, error) {
-	span, ctx := tracing.StartSpanFromContext(a.Context())
-	defer span.Finish()
+	ctx := a.Context()
 
 	spec := s.(*ReadGroupPhysSpec)
 
@@ -238,15 +208,9 @@ func createReadGroupSource(s plan.ProcedureSpec, id execute.DatasetID, a execute
 		return nil, errors.New("nil bounds passed to from")
 	}
 
-	deps := GetStorageDependencies(a.Context()).FromDeps
-
-	req := query.RequestFromContext(a.Context())
-	if req == nil {
-		return nil, errors.New("missing request on context")
-	}
+	deps := GetStorageDependencies(a.Context())
 
-	orgID := req.OrganizationID
-	bucketID, err := spec.LookupBucketID(ctx, orgID, deps.BucketLookup)
+	db, rp, err := spec.LookupDatabase(ctx, deps, a)
 	if err != nil {
 		return nil, err
 	}
@@ -260,10 +224,10 @@ func createReadGroupSource(s plan.ProcedureSpec, id execute.DatasetID, a execute
 		deps.Reader,
 		ReadGroupSpec{
 			ReadFilterSpec: ReadFilterSpec{
-				OrganizationID: orgID,
-				BucketID:       bucketID,
-				Bounds:         *bounds,
-				Predicate:      filter,
+				Database:        db,
+				RetentionPolicy: rp,
+				Bounds:          *bounds,
+				Predicate:       filter,
 			},
 			GroupMode:       ToGroupMode(spec.GroupMode),
 			GroupKeys:       spec.GroupKeys,
@@ -274,18 +238,12 @@ func createReadGroupSource(s plan.ProcedureSpec, id execute.DatasetID, a execute
 }
 
 func createReadTagKeysSource(prSpec plan.ProcedureSpec, dsid execute.DatasetID, a execute.Administration) (execute.Source, error) {
-	span, ctx := tracing.StartSpanFromContext(a.Context())
-	defer span.Finish()
+	ctx := a.Context()
 
 	spec := prSpec.(*ReadTagKeysPhysSpec)
-	deps := GetStorageDependencies(a.Context()).FromDeps
-	req := query.RequestFromContext(a.Context())
-	if req == nil {
-		return nil, errors.New("missing request on context")
-	}
-	orgID := req.OrganizationID
+	deps := GetStorageDependencies(a.Context())
 
-	bucketID, err := spec.LookupBucketID(ctx, orgID, deps.BucketLookup)
+	db, rp, err := spec.LookupDatabase(ctx, deps, a)
 	if err != nil {
 		return nil, err
 	}
@@ -301,10 +259,10 @@ func createReadTagKeysSource(prSpec plan.ProcedureSpec, dsid execute.DatasetID,
 		deps.Reader,
 		ReadTagKeysSpec{
 			ReadFilterSpec: ReadFilterSpec{
-				OrganizationID: orgID,
-				BucketID:       bucketID,
-				Bounds:         *bounds,
-				Predicate:      filter,
+				Database:        db,
+				RetentionPolicy: rp,
+				Bounds:          *bounds,
+				Predicate:       filter,
 			},
 		},
 		a,
@@ -326,10 +284,6 @@ func ReadTagKeysSource(id execute.DatasetID, r Reader, readSpec ReadTagKeysSpec,
 	src.id = id
 	src.alloc = a.Allocator()
 
-	src.m = GetStorageDependencies(a.Context()).FromDeps.Metrics
-	src.orgID = readSpec.OrganizationID
-	src.op = "readTagKeys"
-
 	src.runner = src
 	return src
 }
@@ -343,18 +297,12 @@ func (s *readTagKeysSource) run(ctx context.Context) error {
 }
 
 func createReadTagValuesSource(prSpec plan.ProcedureSpec, dsid execute.DatasetID, a execute.Administration) (execute.Source, error) {
-	span, ctx := tracing.StartSpanFromContext(a.Context())
-	defer span.Finish()
+	ctx := a.Context()
 
 	spec := prSpec.(*ReadTagValuesPhysSpec)
-	deps := GetStorageDependencies(a.Context()).FromDeps
-	req := query.RequestFromContext(a.Context())
-	if req == nil {
-		return nil, errors.New("missing request on context")
-	}
-	orgID := req.OrganizationID
+	deps := GetStorageDependencies(a.Context())
 
-	bucketID, err := spec.LookupBucketID(ctx, orgID, deps.BucketLookup)
+	db, rp, err := spec.LookupDatabase(ctx, deps, a)
 	if err != nil {
 		return nil, err
 	}
@@ -370,10 +318,10 @@ func createReadTagValuesSource(prSpec plan.ProcedureSpec, dsid execute.DatasetID
 		deps.Reader,
 		ReadTagValuesSpec{
 			ReadFilterSpec: ReadFilterSpec{
-				OrganizationID: orgID,
-				BucketID:       bucketID,
-				Bounds:         *bounds,
-				Predicate:      filter,
+				Database:        db,
+				RetentionPolicy: rp,
+				Bounds:          *bounds,
+				Predicate:       filter,
 			},
 			TagKey: spec.TagKey,
 		},
@@ -396,10 +344,6 @@ func ReadTagValuesSource(id execute.DatasetID, r Reader, readSpec ReadTagValuesS
 	src.id = id
 	src.alloc = a.Allocator()
 
-	src.m = GetStorageDependencies(a.Context()).FromDeps.Metrics
-	src.orgID = readSpec.OrganizationID
-	src.op = "readTagValues"
-
 	src.runner = src
 	return src
 }
diff --git b/flux/stdlib/influxdata/influxdb/source_test.go a/flux/stdlib/influxdata/influxdb/source_test.go
index 1cdda0f93..daba8c936 100644
--- b/flux/stdlib/influxdata/influxdb/source_test.go
+++ a/flux/stdlib/influxdata/influxdb/source_test.go
@@ -1,131 +1 @@
 package influxdb_test
-
-import (
-	"context"
-	"testing"
-	"time"
-
-	"github.com/influxdata/flux"
-	"github.com/influxdata/flux/dependencies/dependenciestest"
-	"github.com/influxdata/flux/execute"
-	"github.com/influxdata/flux/memory"
-	platform "github.com/influxdata/influxdb"
-	"github.com/influxdata/influxdb/kit/prom/promtest"
-	"github.com/influxdata/influxdb/mock"
-	"github.com/influxdata/influxdb/query/stdlib/influxdata/influxdb"
-	"github.com/influxdata/influxdb/tsdb/cursors"
-	"github.com/influxdata/influxdb/uuid"
-	"github.com/prometheus/client_golang/prometheus"
-)
-
-type mockTableIterator struct {
-}
-
-func (mockTableIterator) Do(f func(flux.Table) error) error {
-	return nil
-}
-
-func (mockTableIterator) Statistics() cursors.CursorStats {
-	return cursors.CursorStats{}
-}
-
-type mockReader struct {
-}
-
-func (mockReader) ReadFilter(ctx context.Context, spec influxdb.ReadFilterSpec, alloc *memory.Allocator) (influxdb.TableIterator, error) {
-	return &mockTableIterator{}, nil
-}
-
-func (mockReader) ReadGroup(ctx context.Context, spec influxdb.ReadGroupSpec, alloc *memory.Allocator) (influxdb.TableIterator, error) {
-	return &mockTableIterator{}, nil
-}
-
-func (mockReader) ReadTagKeys(ctx context.Context, spec influxdb.ReadTagKeysSpec, alloc *memory.Allocator) (influxdb.TableIterator, error) {
-	return &mockTableIterator{}, nil
-}
-
-func (mockReader) ReadTagValues(ctx context.Context, spec influxdb.ReadTagValuesSpec, alloc *memory.Allocator) (influxdb.TableIterator, error) {
-	return &mockTableIterator{}, nil
-}
-
-func (mockReader) Close() {
-}
-
-type mockAdministration struct {
-	Ctx context.Context
-}
-
-func (a mockAdministration) Context() context.Context {
-	return a.Ctx
-}
-
-func (mockAdministration) ResolveTime(qt flux.Time) execute.Time {
-	return 0
-}
-
-func (mockAdministration) StreamContext() execute.StreamContext {
-	return nil
-}
-
-func (mockAdministration) Allocator() *memory.Allocator {
-	return &memory.Allocator{}
-}
-
-func (mockAdministration) Parents() []execute.DatasetID {
-	return nil
-}
-
-const (
-	labelKey   = "key1"
-	labelValue = "value1"
-)
-
-// TestMetrics ensures that the metrics collected by an influxdb source are recorded.
-func TestMetrics(t *testing.T) {
-	reg := prometheus.NewRegistry()
-
-	orgID, err := platform.IDFromString("deadbeefbeefdead")
-	if err != nil {
-		t.Fatal(err)
-	}
-
-	deps := influxdb.Dependencies{
-		FluxDeps: dependenciestest.Default(),
-		StorageDeps: influxdb.StorageDependencies{
-			FromDeps: influxdb.FromDependencies{
-				Reader:             &mockReader{},
-				BucketLookup:       mock.BucketLookup{},
-				OrganizationLookup: mock.OrganizationLookup{},
-				Metrics:            influxdb.NewMetrics([]string{labelKey}),
-			},
-		},
-	}
-	reg.MustRegister(deps.PrometheusCollectors()...)
-
-	// This key/value pair added to the context will appear as a label in the prometheus histogram.
-	ctx := context.WithValue(context.Background(), labelKey, labelValue) //lint:ignore SA1029 this is a temporary ignore until we have time to create an appropriate type
-	// Injecting deps
-	ctx = deps.Inject(ctx)
-	a := &mockAdministration{Ctx: ctx}
-	rfs := influxdb.ReadFilterSource(
-		execute.DatasetID(uuid.FromTime(time.Now())),
-		&mockReader{},
-		influxdb.ReadFilterSpec{
-			OrganizationID: *orgID,
-		},
-		a,
-	)
-	rfs.Run(ctx)
-
-	// Verify that we sampled the execution of the source by checking the prom registry.
-	mfs := promtest.MustGather(t, reg)
-	expectedLabels := map[string]string{
-		"org":  "deadbeefbeefdead",
-		"key1": "value1",
-		"op":   "readFilter",
-	}
-	m := promtest.MustFindMetric(t, mfs, "query_influxdb_source_read_request_duration_seconds", expectedLabels)
-	if want, got := uint64(1), *(m.Histogram.SampleCount); want != got {
-		t.Fatalf("expected sample count of %v, got %v", want, got)
-	}
-}
diff --git b/flux/stdlib/influxdata/influxdb/storage.go a/flux/stdlib/influxdata/influxdb/storage.go
index 4f574fb20..727894907 100644
--- b/flux/stdlib/influxdata/influxdb/storage.go
+++ a/flux/stdlib/influxdata/influxdb/storage.go
@@ -8,75 +8,33 @@ import (
 	"github.com/influxdata/flux/execute"
 	"github.com/influxdata/flux/memory"
 	"github.com/influxdata/flux/semantic"
-	platform "github.com/influxdata/influxdb"
-	"github.com/influxdata/influxdb/kit/prom"
+	"github.com/influxdata/influxdb/services/meta"
 	"github.com/influxdata/influxdb/tsdb/cursors"
+	"github.com/influxdata/influxql"
 	"github.com/pkg/errors"
-	"github.com/prometheus/client_golang/prometheus"
 )
 
-type HostLookup interface {
-	Hosts() []string
-	Watch() <-chan struct{}
-}
-
-type BucketLookup interface {
-	Lookup(ctx context.Context, orgID platform.ID, name string) (platform.ID, bool)
-	LookupName(ctx context.Context, orgID platform.ID, id platform.ID) string
-}
-
-type OrganizationLookup interface {
-	Lookup(ctx context.Context, name string) (platform.ID, bool)
-	LookupName(ctx context.Context, id platform.ID) string
+type Authorizer interface {
+	AuthorizeDatabase(u meta.User, priv influxql.Privilege, database string) error
 }
 
 type FromDependencies struct {
-	Reader             Reader
-	BucketLookup       BucketLookup
-	OrganizationLookup OrganizationLookup
-	Metrics            *metrics
+	Reader      Reader
+	MetaClient  MetaClient
+	Authorizer  Authorizer
+	AuthEnabled bool
 }
 
 func (d FromDependencies) Validate() error {
 	if d.Reader == nil {
 		return errors.New("missing reader dependency")
 	}
-	if d.BucketLookup == nil {
-		return errors.New("missing bucket lookup dependency")
-	}
-	if d.OrganizationLookup == nil {
-		return errors.New("missing organization lookup dependency")
-	}
-	return nil
-}
-
-// PrometheusCollectors satisfies the PrometheusCollector interface.
-func (d FromDependencies) PrometheusCollectors() []prometheus.Collector {
-	collectors := make([]prometheus.Collector, 0)
-	if pc, ok := d.Reader.(prom.PrometheusCollector); ok {
-		collectors = append(collectors, pc.PrometheusCollectors()...)
+	if d.MetaClient == nil {
+		return errors.New("missing meta client dependency")
 	}
-	if d.Metrics != nil {
-		collectors = append(collectors, d.Metrics.PrometheusCollectors()...)
+	if d.AuthEnabled && d.Authorizer == nil {
+		return errors.New("missing authorizer dependency")
 	}
-	return collectors
-}
-
-type StaticLookup struct {
-	hosts []string
-}
-
-func NewStaticLookup(hosts []string) StaticLookup {
-	return StaticLookup{
-		hosts: hosts,
-	}
-}
-
-func (l StaticLookup) Hosts() []string {
-	return l.hosts
-}
-func (l StaticLookup) Watch() <-chan struct{} {
-	// A nil channel always blocks, since hosts never change this is appropriate.
 	return nil
 }
 
@@ -102,8 +60,8 @@ func ToGroupMode(fluxMode flux.GroupMode) GroupMode {
 }
 
 type ReadFilterSpec struct {
-	OrganizationID platform.ID
-	BucketID       platform.ID
+	Database        string
+	RetentionPolicy string
 
 	Bounds execute.Bounds
 
diff --git b/flux/stdlib/influxdata/influxdb/v1/databases.go a/flux/stdlib/influxdata/influxdb/v1/databases.go
index 6a6c59a76..1779f411c 100644
--- b/flux/stdlib/influxdata/influxdb/v1/databases.go
+++ a/flux/stdlib/influxdata/influxdb/v1/databases.go
@@ -2,8 +2,8 @@ package v1
 
 import (
 	"context"
+	"errors"
 	"fmt"
-	"time"
 
 	"github.com/influxdata/flux"
 	"github.com/influxdata/flux/execute"
@@ -11,9 +11,9 @@ import (
 	"github.com/influxdata/flux/plan"
 	"github.com/influxdata/flux/stdlib/influxdata/influxdb/v1"
 	"github.com/influxdata/flux/values"
-	platform "github.com/influxdata/influxdb"
-	"github.com/influxdata/influxdb/query"
-	"github.com/pkg/errors"
+	"github.com/influxdata/influxdb/flux/stdlib/influxdata/influxdb"
+	"github.com/influxdata/influxdb/services/meta"
+	"github.com/influxdata/influxql"
 )
 
 const DatabasesKind = v1.DatabasesKind
@@ -67,9 +67,9 @@ func init() {
 }
 
 type DatabasesDecoder struct {
-	orgID     platform.ID
-	deps      *DatabasesDependencies
-	databases []*platform.DBRPMapping
+	deps      *influxdb.StorageDependencies
+	databases []meta.DatabaseInfo
+	user      meta.User
 	alloc     *memory.Allocator
 }
 
@@ -78,45 +78,13 @@ func (bd *DatabasesDecoder) Connect(ctx context.Context) error {
 }
 
 func (bd *DatabasesDecoder) Fetch(ctx context.Context) (bool, error) {
-	b, _, err := bd.deps.DBRP.FindMany(ctx, platform.DBRPMappingFilter{})
-	if err != nil {
-		return false, err
-	}
-	bd.databases = b
+	bd.databases = bd.deps.MetaClient.Databases()
 	return false, nil
 }
 
 func (bd *DatabasesDecoder) Decode(ctx context.Context) (flux.Table, error) {
-	type databaseInfo struct {
-		*platform.DBRPMapping
-		RetentionPeriod time.Duration
-	}
-
-	databases := make([]databaseInfo, 0, len(bd.databases))
-	for _, db := range bd.databases {
-		bucket, err := bd.deps.BucketLookup.FindBucketByID(ctx, db.BucketID)
-		if err != nil {
-			code := platform.ErrorCode(err)
-			if code == platform.EUnauthorized || code == platform.EForbidden {
-				continue
-			}
-			return nil, err
-		}
-		databases = append(databases, databaseInfo{
-			DBRPMapping:     db,
-			RetentionPeriod: bucket.RetentionPeriod,
-		})
-	}
-
-	if len(databases) == 0 {
-		return nil, &platform.Error{
-			Code: platform.ENotFound,
-			Msg:  "no 1.x databases found",
-		}
-	}
-
 	kb := execute.NewGroupKeyBuilder(nil)
-	kb.AddKeyValue("organizationID", values.NewString(databases[0].OrganizationID.String()))
+	kb.AddKeyValue("organizationID", values.NewString(""))
 	gk, err := kb.Build()
 	if err != nil {
 		return nil, err
@@ -160,13 +128,29 @@ func (bd *DatabasesDecoder) Decode(ctx context.Context) (flux.Table, error) {
 		return nil, err
 	}
 
-	for _, db := range databases {
-		_ = b.AppendString(0, db.OrganizationID.String())
-		_ = b.AppendString(1, db.Database)
-		_ = b.AppendString(2, db.RetentionPolicy)
-		_ = b.AppendInt(3, db.RetentionPeriod.Nanoseconds())
-		_ = b.AppendBool(4, db.Default)
-		_ = b.AppendString(5, db.BucketID.String())
+	var hasAccess func(db string) bool
+	if bd.user == nil {
+		hasAccess = func(db string) bool {
+			return true
+		}
+	} else {
+		hasAccess = func(db string) bool {
+			return bd.deps.Authorizer.AuthorizeDatabase(bd.user, influxql.ReadPrivilege, db) == nil ||
+				bd.deps.Authorizer.AuthorizeDatabase(bd.user, influxql.WritePrivilege, db) == nil
+		}
+	}
+
+	for _, db := range bd.databases {
+		if hasAccess(db.Name) {
+			for _, rp := range db.RetentionPolicies {
+				_ = b.AppendString(0, "")
+				_ = b.AppendString(1, db.Name)
+				_ = b.AppendString(2, rp.Name)
+				_ = b.AppendInt(3, rp.Duration.Nanoseconds())
+				_ = b.AppendBool(4, db.DefaultRetentionPolicy == rp.Name)
+				_ = b.AppendString(5, "")
+			}
+		}
 	}
 
 	return b.Table()
@@ -181,41 +165,14 @@ func createDatabasesSource(prSpec plan.ProcedureSpec, dsid execute.DatasetID, a
 	if !ok {
 		return nil, fmt.Errorf("invalid spec type %T", prSpec)
 	}
-	deps := GetDatabasesDependencies(a.Context())
-	req := query.RequestFromContext(a.Context())
-	if req == nil {
-		return nil, errors.New("missing request on context")
+	deps := influxdb.GetStorageDependencies(a.Context())
+	var user meta.User
+	if deps.AuthEnabled {
+		user = meta.UserFromContext(a.Context())
+		if user == nil {
+			return nil, errors.New("createDatabasesSource: no user")
+		}
 	}
-	orgID := req.OrganizationID
-
-	bd := &DatabasesDecoder{orgID: orgID, deps: &deps, alloc: a.Allocator()}
-
+	bd := &DatabasesDecoder{deps: &deps, alloc: a.Allocator(), user: user}
 	return execute.CreateSourceFromDecoder(bd, dsid, a)
 }
-
-type key int
-
-const dependenciesKey key = iota
-
-type DatabasesDependencies struct {
-	DBRP         platform.DBRPMappingService
-	BucketLookup platform.BucketService
-}
-
-func (d DatabasesDependencies) Inject(ctx context.Context) context.Context {
-	return context.WithValue(ctx, dependenciesKey, d)
-}
-
-func GetDatabasesDependencies(ctx context.Context) DatabasesDependencies {
-	return ctx.Value(dependenciesKey).(DatabasesDependencies)
-}
-
-func (d DatabasesDependencies) Validate() error {
-	if d.DBRP == nil {
-		return errors.New("missing all databases lookup dependency")
-	}
-	if d.BucketLookup == nil {
-		return errors.New("missing buckets lookup dependency")
-	}
-	return nil
-}
diff --git b/patches/flux.patch a/patches/flux.patch
index a35a722a5..743b1ddd2 100644
--- b/patches/flux.patch
+++ a/patches/flux.patch
@@ -0,0 +1,905 @@
+diff -ur a/flux/stdlib/influxdata/influxdb/buckets.go b/flux/stdlib/influxdata/influxdb/buckets.go
+--- a/flux/stdlib/influxdata/influxdb/buckets.go	2019-06-20 14:35:12.000000000 -0500
++++ b/flux/stdlib/influxdata/influxdb/buckets.go	2019-06-25 10:03:53.000000000 -0500
+@@ -1,6 +1,7 @@
+ package influxdb
+ 
+ import (
++	"errors"
+ 	"fmt"
+ 
+ 	"github.com/influxdata/flux"
+@@ -9,9 +10,8 @@
+ 	"github.com/influxdata/flux/plan"
+ 	"github.com/influxdata/flux/stdlib/influxdata/influxdb"
+ 	"github.com/influxdata/flux/values"
+-	platform "github.com/influxdata/influxdb"
+-	"github.com/influxdata/influxdb/query"
+-	"github.com/pkg/errors"
++	"github.com/influxdata/influxdb/services/meta"
++	"github.com/influxdata/influxql"
+ )
+ 
+ func init() {
+@@ -19,10 +19,9 @@
+ }
+ 
+ type BucketsDecoder struct {
+-	orgID   platform.ID
+-	deps    BucketDependencies
+-	buckets []*platform.Bucket
+-	alloc   *memory.Allocator
++	deps  BucketDependencies
++	alloc *memory.Allocator
++	user  meta.User
+ }
+ 
+ func (bd *BucketsDecoder) Connect() error {
+@@ -30,17 +29,12 @@
+ }
+ 
+ func (bd *BucketsDecoder) Fetch() (bool, error) {
+-	b, count := bd.deps.FindAllBuckets(bd.orgID)
+-	if count <= 0 {
+-		return false, fmt.Errorf("no buckets found in organization %v", bd.orgID)
+-	}
+-	bd.buckets = b
+ 	return false, nil
+ }
+ 
+ func (bd *BucketsDecoder) Decode() (flux.Table, error) {
+ 	kb := execute.NewGroupKeyBuilder(nil)
+-	kb.AddKeyValue("organizationID", values.NewString(bd.buckets[0].OrgID.String()))
++	kb.AddKeyValue("organizationID", values.NewString(""))
+ 	gk, err := kb.Build()
+ 	if err != nil {
+ 		return nil, err
+@@ -48,43 +42,54 @@
+ 
+ 	b := execute.NewColListTableBuilder(gk, bd.alloc)
+ 
+-	if _, err := b.AddCol(flux.ColMeta{
++	_, _ = b.AddCol(flux.ColMeta{
+ 		Label: "name",
+ 		Type:  flux.TString,
+-	}); err != nil {
+-		return nil, err
+-	}
+-	if _, err := b.AddCol(flux.ColMeta{
++	})
++	_, _ = b.AddCol(flux.ColMeta{
+ 		Label: "id",
+ 		Type:  flux.TString,
+-	}); err != nil {
+-		return nil, err
+-	}
+-	if _, err := b.AddCol(flux.ColMeta{
++	})
++	_, _ = b.AddCol(flux.ColMeta{
++		Label: "organization",
++		Type:  flux.TString,
++	})
++	_, _ = b.AddCol(flux.ColMeta{
+ 		Label: "organizationID",
+ 		Type:  flux.TString,
+-	}); err != nil {
+-		return nil, err
+-	}
+-	if _, err := b.AddCol(flux.ColMeta{
++	})
++	_, _ = b.AddCol(flux.ColMeta{
+ 		Label: "retentionPolicy",
+ 		Type:  flux.TString,
+-	}); err != nil {
+-		return nil, err
+-	}
+-	if _, err := b.AddCol(flux.ColMeta{
++	})
++	_, _ = b.AddCol(flux.ColMeta{
+ 		Label: "retentionPeriod",
+ 		Type:  flux.TInt,
+-	}); err != nil {
+-		return nil, err
+-	}
++	})
+ 
+-	for _, bucket := range bd.buckets {
+-		_ = b.AppendString(0, bucket.Name)
+-		_ = b.AppendString(1, bucket.ID.String())
+-		_ = b.AppendString(2, bucket.OrgID.String())
+-		_ = b.AppendString(3, bucket.RetentionPolicyName)
+-		_ = b.AppendInt(4, bucket.RetentionPeriod.Nanoseconds())
++	var hasAccess func(db string) bool
++	if bd.user == nil {
++		hasAccess = func(db string) bool {
++			return true
++		}
++	} else {
++		hasAccess = func(db string) bool {
++			return bd.deps.Authorizer.AuthorizeDatabase(bd.user, influxql.ReadPrivilege, db) == nil ||
++				bd.deps.Authorizer.AuthorizeDatabase(bd.user, influxql.WritePrivilege, db) == nil
++		}
++	}
++
++	for _, db := range bd.deps.MetaClient.Databases() {
++		if hasAccess(db.Name) {
++			for _, rp := range db.RetentionPolicies {
++				_ = b.AppendString(0, db.Name+"/"+rp.Name)
++				_ = b.AppendString(1, "")
++				_ = b.AppendString(2, "influxdb")
++				_ = b.AppendString(3, "")
++				_ = b.AppendString(4, rp.Name)
++				_ = b.AppendInt(5, rp.Duration.Nanoseconds())
++			}
++		}
+ 	}
+ 
+ 	return b.Table()
+@@ -103,25 +108,45 @@
+ 	// the dependencies used for FromKind are adequate for what we need here
+ 	// so there's no need to inject custom dependencies for buckets()
+ 	deps := a.Dependencies()[influxdb.BucketsKind].(BucketDependencies)
+-	req := query.RequestFromContext(a.Context())
+-	if req == nil {
+-		return nil, errors.New("missing request on context")
++
++	var user meta.User
++	if deps.AuthEnabled {
++		user = meta.UserFromContext(a.Context())
++		if user == nil {
++			return nil, errors.New("createBucketsSource: no user")
++		}
+ 	}
+-	orgID := req.OrganizationID
+ 
+-	bd := &BucketsDecoder{orgID: orgID, deps: deps, alloc: a.Allocator()}
++	bd := &BucketsDecoder{deps: deps, alloc: a.Allocator(), user: user}
+ 
+ 	return execute.CreateSourceFromDecoder(bd, dsid, a)
++
++}
++
++type MetaClient interface {
++	Databases() []meta.DatabaseInfo
++	Database(name string) *meta.DatabaseInfo
+ }
+ 
+-type AllBucketLookup interface {
+-	FindAllBuckets(orgID platform.ID) ([]*platform.Bucket, int)
++type BucketDependencies struct {
++	MetaClient  MetaClient
++	Authorizer  Authorizer
++	AuthEnabled bool
++}
++
++func (d BucketDependencies) Validate() error {
++	if d.MetaClient == nil {
++		return errors.New("validate BucketDependencies: missing MetaClient")
++	}
++	if d.AuthEnabled && d.Authorizer == nil {
++		return errors.New("validate BucketDependencies: missing Authorizer")
++	}
++	return nil
+ }
+-type BucketDependencies AllBucketLookup
+ 
+ func InjectBucketDependencies(depsMap execute.Dependencies, deps BucketDependencies) error {
+-	if deps == nil {
+-		return errors.New("missing all bucket lookup dependency")
++	if err := deps.Validate(); err != nil {
++		return err
+ 	}
+ 	depsMap[influxdb.BucketsKind] = deps
+ 	return nil
+diff -ur a/flux/stdlib/influxdata/influxdb/from.go b/flux/stdlib/influxdata/influxdb/from.go
+--- a/flux/stdlib/influxdata/influxdb/from.go	2019-06-20 14:35:12.000000000 -0500
++++ b/flux/stdlib/influxdata/influxdb/from.go	2019-06-25 13:49:49.000000000 -0500
+@@ -8,7 +8,6 @@
+ 	"github.com/influxdata/flux/plan"
+ 	"github.com/influxdata/flux/semantic"
+ 	"github.com/influxdata/flux/stdlib/influxdata/influxdb"
+-	platform "github.com/influxdata/influxdb"
+ 	"github.com/pkg/errors"
+ )
+ 
+@@ -66,31 +65,6 @@
+ 	return FromKind
+ }
+ 
+-// BucketsAccessed makes FromOpSpec a query.BucketAwareOperationSpec
+-func (s *FromOpSpec) BucketsAccessed(orgID *platform.ID) (readBuckets, writeBuckets []platform.BucketFilter) {
+-	bf := platform.BucketFilter{}
+-	if s.Bucket != "" {
+-		bf.Name = &s.Bucket
+-	}
+-	if orgID != nil {
+-		bf.OrganizationID = orgID
+-	}
+-
+-	if len(s.BucketID) > 0 {
+-		if id, err := platform.IDFromString(s.BucketID); err != nil {
+-			invalidID := platform.InvalidID()
+-			bf.ID = &invalidID
+-		} else {
+-			bf.ID = id
+-		}
+-	}
+-
+-	if bf.ID != nil || bf.Name != nil {
+-		readBuckets = append(readBuckets, bf)
+-	}
+-	return readBuckets, writeBuckets
+-}
+-
+ type FromProcedureSpec struct {
+ 	Bucket   string
+ 	BucketID string
+diff -ur a/flux/stdlib/influxdata/influxdb/operators.go b/flux/stdlib/influxdata/influxdb/operators.go
+--- a/flux/stdlib/influxdata/influxdb/operators.go	2019-06-20 14:35:12.000000000 -0500
++++ b/flux/stdlib/influxdata/influxdb/operators.go	2019-06-25 13:49:49.000000000 -0500
+@@ -3,13 +3,15 @@
+ import (
+ 	"context"
+ 	"errors"
+-	"fmt"
++	"strings"
+ 
+ 	"github.com/influxdata/flux"
++	"github.com/influxdata/flux/execute"
+ 	"github.com/influxdata/flux/plan"
+ 	"github.com/influxdata/flux/semantic"
+ 	"github.com/influxdata/flux/values"
+-	"github.com/influxdata/influxdb"
++	"github.com/influxdata/influxdb/services/meta"
++	"github.com/influxdata/influxql"
+ )
+ 
+ const (
+@@ -79,24 +81,43 @@
+ 	return ns
+ }
+ 
+-func (s *ReadRangePhysSpec) LookupBucketID(ctx context.Context, orgID influxdb.ID, buckets BucketLookup) (influxdb.ID, error) {
+-	// Determine bucketID
+-	switch {
+-	case s.Bucket != "":
+-		b, ok := buckets.Lookup(ctx, orgID, s.Bucket)
+-		if !ok {
+-			return 0, fmt.Errorf("could not find bucket %q", s.Bucket)
++func (s *ReadRangePhysSpec) LookupDatabase(ctx context.Context, deps Dependencies, a execute.Administration) (string, string, error) {
++	if len(s.BucketID) != 0 {
++		return "", "", errors.New("cannot refer to buckets by their id in 1.x")
++	}
++
++	var db, rp string
++	if i := strings.IndexByte(s.Bucket, '/'); i == -1 {
++		db = s.Bucket
++	} else {
++		rp = s.Bucket[i+1:]
++		db = s.Bucket[:i]
++	}
++
++	// validate and resolve db/rp
++	di := deps.MetaClient.Database(db)
++	if di == nil {
++		return "", "", errors.New("no database")
++	}
++
++	if deps.AuthEnabled {
++		user := meta.UserFromContext(a.Context())
++		if user == nil {
++			return "", "", errors.New("createFromSource: no user")
+ 		}
+-		return b, nil
+-	case len(s.BucketID) != 0:
+-		var b influxdb.ID
+-		if err := b.DecodeFromString(s.BucketID); err != nil {
+-			return 0, err
++		if err := deps.Authorizer.AuthorizeDatabase(user, influxql.ReadPrivilege, db); err != nil {
++			return "", "", err
+ 		}
+-		return b, nil
+-	default:
+-		return 0, errors.New("no bucket name or id have been specified")
+ 	}
++
++	if rp == "" {
++		rp = di.DefaultRetentionPolicy
++	}
++
++	if rpi := di.RetentionPolicy(rp); rpi == nil {
++		return "", "", errors.New("invalid retention policy")
++	}
++	return db, rp, nil
+ }
+ 
+ // TimeBounds implements plan.BoundsAwareProcedureSpec.
+diff -ur a/flux/stdlib/influxdata/influxdb/rules.go b/flux/stdlib/influxdata/influxdb/rules.go
+--- a/flux/stdlib/influxdata/influxdb/rules.go	2019-06-20 14:35:12.000000000 -0500
++++ b/flux/stdlib/influxdata/influxdb/rules.go	2019-06-25 13:49:50.000000000 -0500
+@@ -190,6 +190,12 @@
+ 	// constructing our own replacement. We do not care about it
+ 	// at the moment though which is why it is not in the pattern.
+ 
++	// The tag keys mechanism doesn't know about fields so we cannot
++	// push down _field comparisons in 1.x.
++	if hasFieldExpr(fromSpec.Filter) {
++		return pn, false, nil
++	}
++
+ 	// The schema mutator needs to correspond to a keep call
+ 	// on the column specified by the keys procedure.
+ 	if len(keepSpec.Mutations) != 1 {
+@@ -219,6 +225,20 @@
+ 	}), true, nil
+ }
+ 
++func hasFieldExpr(expr semantic.Expression) bool {
++	hasField := false
++	v := semantic.CreateVisitor(func(node semantic.Node) {
++		switch n := node.(type) {
++		case *semantic.MemberExpression:
++			if n.Property == "_field" {
++				hasField = true
++			}
++		}
++	})
++	semantic.Walk(v, expr)
++	return hasField
++}
++
+ // PushDownReadTagValuesRule matches 'ReadRange |> keep(columns: [tag]) |> group() |> distinct(column: tag)'.
+ // The 'from()' must have already been merged with 'range' and, optionally,
+ // may have been merged with 'filter'.
+@@ -296,6 +316,9 @@
+ 	execute.DefaultValueColLabel,
+ 	execute.DefaultStartColLabel,
+ 	execute.DefaultStopColLabel,
++	// TODO(jsternberg): There just doesn't seem to be a good way to do this
++	// in the 1.x line of the release.
++	"_field",
+ }
+ 
+ // isValidTagKeyForTagValues returns true if the given key can
+diff -ur a/flux/stdlib/influxdata/influxdb/rules_test.go b/flux/stdlib/influxdata/influxdb/rules_test.go
+--- a/flux/stdlib/influxdata/influxdb/rules_test.go	2019-06-20 14:35:12.000000000 -0500
++++ b/flux/stdlib/influxdata/influxdb/rules_test.go	2019-06-25 13:49:49.000000000 -0500
+@@ -11,7 +11,7 @@
+ 	"github.com/influxdata/flux/plan/plantest"
+ 	"github.com/influxdata/flux/semantic"
+ 	"github.com/influxdata/flux/stdlib/universe"
+-	"github.com/influxdata/influxdb/query/stdlib/influxdata/influxdb"
++	"github.com/influxdata/influxdb/flux/stdlib/influxdata/influxdb"
+ )
+ 
+ func fluxTime(t int64) flux.Time {
+diff -ur a/flux/stdlib/influxdata/influxdb/source.go b/flux/stdlib/influxdata/influxdb/source.go
+--- a/flux/stdlib/influxdata/influxdb/source.go	2019-06-20 14:35:12.000000000 -0500
++++ b/flux/stdlib/influxdata/influxdb/source.go	2019-06-25 13:49:49.000000000 -0500
+@@ -9,8 +9,6 @@
+ 	"github.com/influxdata/flux/memory"
+ 	"github.com/influxdata/flux/plan"
+ 	"github.com/influxdata/flux/semantic"
+-	"github.com/influxdata/influxdb/kit/tracing"
+-	"github.com/influxdata/influxdb/query"
+ 	"github.com/influxdata/influxdb/tsdb/cursors"
+ )
+ 
+@@ -131,8 +129,7 @@
+ }
+ 
+ func createReadFilterSource(s plan.ProcedureSpec, id execute.DatasetID, a execute.Administration) (execute.Source, error) {
+-	span, ctx := tracing.StartSpanFromContext(a.Context())
+-	defer span.Finish()
++	ctx := a.Context()
+ 
+ 	spec := s.(*ReadRangePhysSpec)
+ 
+@@ -143,13 +140,7 @@
+ 
+ 	deps := a.Dependencies()[FromKind].(Dependencies)
+ 
+-	req := query.RequestFromContext(a.Context())
+-	if req == nil {
+-		return nil, errors.New("missing request on context")
+-	}
+-
+-	orgID := req.OrganizationID
+-	bucketID, err := spec.LookupBucketID(ctx, orgID, deps.BucketLookup)
++	db, rp, err := spec.LookupDatabase(ctx, deps, a)
+ 	if err != nil {
+ 		return nil, err
+ 	}
+@@ -162,10 +153,10 @@
+ 		id,
+ 		deps.Reader,
+ 		ReadFilterSpec{
+-			OrganizationID: orgID,
+-			BucketID:       bucketID,
+-			Bounds:         *bounds,
+-			Predicate:      filter,
++			Database:        db,
++			RetentionPolicy: rp,
++			Bounds:          *bounds,
++			Predicate:       filter,
+ 		},
+ 		a.Allocator(),
+ 	), nil
+@@ -204,8 +195,7 @@
+ }
+ 
+ func createReadGroupSource(s plan.ProcedureSpec, id execute.DatasetID, a execute.Administration) (execute.Source, error) {
+-	span, ctx := tracing.StartSpanFromContext(a.Context())
+-	defer span.Finish()
++	ctx := a.Context()
+ 
+ 	spec := s.(*ReadGroupPhysSpec)
+ 
+@@ -216,13 +206,7 @@
+ 
+ 	deps := a.Dependencies()[FromKind].(Dependencies)
+ 
+-	req := query.RequestFromContext(a.Context())
+-	if req == nil {
+-		return nil, errors.New("missing request on context")
+-	}
+-
+-	orgID := req.OrganizationID
+-	bucketID, err := spec.LookupBucketID(ctx, orgID, deps.BucketLookup)
++	db, rp, err := spec.LookupDatabase(ctx, deps, a)
+ 	if err != nil {
+ 		return nil, err
+ 	}
+@@ -236,10 +220,10 @@
+ 		deps.Reader,
+ 		ReadGroupSpec{
+ 			ReadFilterSpec: ReadFilterSpec{
+-				OrganizationID: orgID,
+-				BucketID:       bucketID,
+-				Bounds:         *bounds,
+-				Predicate:      filter,
++				Database:        db,
++				RetentionPolicy: rp,
++				Bounds:          *bounds,
++				Predicate:       filter,
+ 			},
+ 			GroupMode:       ToGroupMode(spec.GroupMode),
+ 			GroupKeys:       spec.GroupKeys,
+@@ -250,18 +234,12 @@
+ }
+ 
+ func createReadTagKeysSource(prSpec plan.ProcedureSpec, dsid execute.DatasetID, a execute.Administration) (execute.Source, error) {
+-	span, ctx := tracing.StartSpanFromContext(a.Context())
+-	defer span.Finish()
++	ctx := a.Context()
+ 
+ 	spec := prSpec.(*ReadTagKeysPhysSpec)
+ 	deps := a.Dependencies()[FromKind].(Dependencies)
+-	req := query.RequestFromContext(a.Context())
+-	if req == nil {
+-		return nil, errors.New("missing request on context")
+-	}
+-	orgID := req.OrganizationID
+ 
+-	bucketID, err := spec.LookupBucketID(ctx, orgID, deps.BucketLookup)
++	db, rp, err := spec.LookupDatabase(ctx, deps, a)
+ 	if err != nil {
+ 		return nil, err
+ 	}
+@@ -277,10 +255,10 @@
+ 		deps.Reader,
+ 		ReadTagKeysSpec{
+ 			ReadFilterSpec: ReadFilterSpec{
+-				OrganizationID: orgID,
+-				BucketID:       bucketID,
+-				Bounds:         *bounds,
+-				Predicate:      filter,
++				Database:        db,
++				RetentionPolicy: rp,
++				Bounds:          *bounds,
++				Predicate:       filter,
+ 			},
+ 		},
+ 		a.Allocator(),
+@@ -314,18 +292,12 @@
+ }
+ 
+ func createReadTagValuesSource(prSpec plan.ProcedureSpec, dsid execute.DatasetID, a execute.Administration) (execute.Source, error) {
+-	span, ctx := tracing.StartSpanFromContext(a.Context())
+-	defer span.Finish()
++	ctx := a.Context()
+ 
+ 	spec := prSpec.(*ReadTagValuesPhysSpec)
+ 	deps := a.Dependencies()[FromKind].(Dependencies)
+-	req := query.RequestFromContext(a.Context())
+-	if req == nil {
+-		return nil, errors.New("missing request on context")
+-	}
+-	orgID := req.OrganizationID
+ 
+-	bucketID, err := spec.LookupBucketID(ctx, orgID, deps.BucketLookup)
++	db, rp, err := spec.LookupDatabase(ctx, deps, a)
+ 	if err != nil {
+ 		return nil, err
+ 	}
+@@ -341,10 +313,10 @@
+ 		deps.Reader,
+ 		ReadTagValuesSpec{
+ 			ReadFilterSpec: ReadFilterSpec{
+-				OrganizationID: orgID,
+-				BucketID:       bucketID,
+-				Bounds:         *bounds,
+-				Predicate:      filter,
++				Database:        db,
++				RetentionPolicy: rp,
++				Bounds:          *bounds,
++				Predicate:       filter,
+ 			},
+ 			TagKey: spec.TagKey,
+ 		},
+diff -ur a/flux/stdlib/influxdata/influxdb/storage.go b/flux/stdlib/influxdata/influxdb/storage.go
+--- a/flux/stdlib/influxdata/influxdb/storage.go	2019-06-20 14:35:12.000000000 -0500
++++ b/flux/stdlib/influxdata/influxdb/storage.go	2019-06-25 13:49:49.000000000 -0500
+@@ -8,61 +8,36 @@
+ 	"github.com/influxdata/flux/execute"
+ 	"github.com/influxdata/flux/memory"
+ 	"github.com/influxdata/flux/semantic"
+-	platform "github.com/influxdata/influxdb"
++	"github.com/influxdata/influxdb/services/meta"
+ 	"github.com/influxdata/influxdb/tsdb/cursors"
++	"github.com/influxdata/influxql"
+ 	"github.com/pkg/errors"
+ )
+ 
+-type HostLookup interface {
+-	Hosts() []string
+-	Watch() <-chan struct{}
+-}
+-
+-type BucketLookup interface {
+-	Lookup(ctx context.Context, orgID platform.ID, name string) (platform.ID, bool)
+-}
+-
+-type OrganizationLookup interface {
+-	Lookup(ctx context.Context, name string) (platform.ID, bool)
++type Authorizer interface {
++	AuthorizeDatabase(u meta.User, priv influxql.Privilege, database string) error
+ }
+ 
+ type Dependencies struct {
+-	Reader             Reader
+-	BucketLookup       BucketLookup
+-	OrganizationLookup OrganizationLookup
++	Reader      Reader
++	MetaClient  MetaClient
++	Authorizer  Authorizer
++	AuthEnabled bool
+ }
+ 
+ func (d Dependencies) Validate() error {
+ 	if d.Reader == nil {
+ 		return errors.New("missing reader dependency")
+ 	}
+-	if d.BucketLookup == nil {
+-		return errors.New("missing bucket lookup dependency")
++	if d.MetaClient == nil {
++		return errors.New("missing meta client dependency")
+ 	}
+-	if d.OrganizationLookup == nil {
+-		return errors.New("missing organization lookup dependency")
++	if d.AuthEnabled && d.Authorizer == nil {
++		return errors.New("validate Dependencies: missing Authorizer")
+ 	}
+ 	return nil
+ }
+ 
+-type StaticLookup struct {
+-	hosts []string
+-}
+-
+-func NewStaticLookup(hosts []string) StaticLookup {
+-	return StaticLookup{
+-		hosts: hosts,
+-	}
+-}
+-
+-func (l StaticLookup) Hosts() []string {
+-	return l.hosts
+-}
+-func (l StaticLookup) Watch() <-chan struct{} {
+-	// A nil channel always blocks, since hosts never change this is appropriate.
+-	return nil
+-}
+-
+ type GroupMode int
+ 
+ const (
+@@ -85,8 +60,8 @@
+ }
+ 
+ type ReadFilterSpec struct {
+-	OrganizationID platform.ID
+-	BucketID       platform.ID
++	Database        string
++	RetentionPolicy string
+ 
+ 	Bounds execute.Bounds
+ 
+diff -ur a/flux/stdlib/influxdata/influxdb/v1/databases.go b/flux/stdlib/influxdata/influxdb/v1/databases.go
+--- a/flux/stdlib/influxdata/influxdb/v1/databases.go	2019-06-20 14:35:12.000000000 -0500
++++ b/flux/stdlib/influxdata/influxdb/v1/databases.go	2019-06-25 10:03:53.000000000 -0500
+@@ -2,17 +2,19 @@
+ 
+ import (
+ 	"context"
++	"errors"
+ 	"fmt"
+ 
+ 	"github.com/influxdata/flux"
+ 	"github.com/influxdata/flux/execute"
+ 	"github.com/influxdata/flux/memory"
+ 	"github.com/influxdata/flux/plan"
+-	"github.com/influxdata/flux/stdlib/influxdata/influxdb/v1"
++	v1 "github.com/influxdata/flux/stdlib/influxdata/influxdb/v1"
+ 	"github.com/influxdata/flux/values"
+-	platform "github.com/influxdata/influxdb"
+-	"github.com/influxdata/influxdb/query"
+-	"github.com/pkg/errors"
++	"github.com/influxdata/influxdb/coordinator"
++	"github.com/influxdata/influxdb/flux/stdlib/influxdata/influxdb"
++	"github.com/influxdata/influxdb/services/meta"
++	"github.com/influxdata/influxql"
+ )
+ 
+ const DatabasesKind = v1.DatabasesKind
+@@ -66,9 +68,9 @@
+ }
+ 
+ type DatabasesDecoder struct {
+-	orgID     platform.ID
+-	deps      *DatabasesDependencies
+-	databases []*platform.DBRPMapping
++	deps      *DatabaseDependencies
++	databases []meta.DatabaseInfo
++	user      meta.User
+ 	alloc     *memory.Allocator
+ 	ctx       context.Context
+ }
+@@ -78,20 +80,13 @@
+ }
+ 
+ func (bd *DatabasesDecoder) Fetch() (bool, error) {
+-	b, _, err := bd.deps.DBRP.FindMany(bd.ctx, platform.DBRPMappingFilter{})
+-	if err != nil {
+-		return false, err
+-	}
+-	bd.databases = b
++	bd.databases = bd.deps.MetaClient.Databases()
+ 	return false, nil
+ }
+ 
+ func (bd *DatabasesDecoder) Decode() (flux.Table, error) {
+ 	kb := execute.NewGroupKeyBuilder(nil)
+-	if len(bd.databases) == 0 {
+-		return nil, errors.New("no 1.x databases found")
+-	}
+-	kb.AddKeyValue("organizationID", values.NewString(bd.databases[0].OrganizationID.String()))
++	kb.AddKeyValue("organizationID", values.NewString(""))
+ 	gk, err := kb.Build()
+ 	if err != nil {
+ 		return nil, err
+@@ -136,16 +131,28 @@
+ 		return nil, err
+ 	}
+ 
++	var hasAccess func(db string) bool
++	if bd.user == nil {
++		hasAccess = func(db string) bool {
++			return true
++		}
++	} else {
++		hasAccess = func(db string) bool {
++			return bd.deps.Authorizer.AuthorizeDatabase(bd.user, influxql.ReadPrivilege, db) == nil ||
++				bd.deps.Authorizer.AuthorizeDatabase(bd.user, influxql.WritePrivilege, db) == nil
++		}
++	}
++
+ 	for _, db := range bd.databases {
+-		if bucket, err := bd.deps.BucketLookup.FindBucketByID(bd.ctx, db.BucketID); err != nil {
+-			return nil, err
+-		} else {
+-			_ = b.AppendString(0, db.OrganizationID.String())
+-			_ = b.AppendString(1, db.Database)
+-			_ = b.AppendString(2, db.RetentionPolicy)
+-			_ = b.AppendInt(3, bucket.RetentionPeriod.Nanoseconds())
+-			_ = b.AppendBool(4, db.Default)
+-			_ = b.AppendString(5, db.BucketID.String())
++		if hasAccess(db.Name) {
++			for _, rp := range db.RetentionPolicies {
++				_ = b.AppendString(0, "")
++				_ = b.AppendString(1, db.Name)
++				_ = b.AppendString(2, rp.Name)
++				_ = b.AppendInt(3, rp.Duration.Nanoseconds())
++				_ = b.AppendBool(4, db.DefaultRetentionPolicy == rp.Name)
++				_ = b.AppendString(5, "")
++			}
+ 		}
+ 	}
+ 
+@@ -162,34 +169,31 @@
+ 		return nil, fmt.Errorf("invalid spec type %T", prSpec)
+ 	}
+ 
+-	// the dependencies used for FromKind are adequate for what we need here
+-	// so there's no need to inject custom dependencies for databases()
+-	deps := a.Dependencies()[DatabasesKind].(DatabasesDependencies)
+-	req := query.RequestFromContext(a.Context())
+-	if req == nil {
+-		return nil, errors.New("missing request on context")
++	deps := a.Dependencies()[DatabasesKind].(DatabaseDependencies)
++	var user meta.User
++	if deps.AuthEnabled {
++		user = meta.UserFromContext(a.Context())
++		if user == nil {
++			return nil, errors.New("createDatabasesSource: no user")
++		}
+ 	}
+-	orgID := req.OrganizationID
+-
+-	bd := &DatabasesDecoder{orgID: orgID, deps: &deps, alloc: a.Allocator(), ctx: a.Context()}
+-
++	bd := &DatabasesDecoder{deps: &deps, alloc: a.Allocator(), ctx: a.Context(), user: user}
+ 	return execute.CreateSourceFromDecoder(bd, dsid, a)
+ }
+ 
+-type DatabasesDependencies struct {
+-	DBRP         platform.DBRPMappingService
+-	BucketLookup platform.BucketService
++type DatabaseDependencies struct {
++	MetaClient  coordinator.MetaClient
++	Authorizer  influxdb.Authorizer
++	AuthEnabled bool
+ }
+ 
+-func InjectDatabasesDependencies(depsMap execute.Dependencies, deps DatabasesDependencies) error {
+-	if deps.DBRP == nil {
+-		return errors.New("missing all databases lookup dependency")
++func InjectDatabaseDependencies(depsMap execute.Dependencies, deps DatabaseDependencies) error {
++	if deps.MetaClient == nil {
++		return errors.New("missing meta client dependency")
+ 	}
+-
+-	if deps.BucketLookup == nil {
+-		return errors.New("missing buckets lookup dependency")
++	if deps.AuthEnabled && deps.Authorizer == nil {
++		return errors.New("missing authorizer with auth enabled")
+ 	}
+-
+ 	depsMap[DatabasesKind] = deps
+ 	return nil
+ }
+diff -ur a/storage/reads/group_resultset.go b/storage/reads/group_resultset.go
+--- a/storage/reads/group_resultset.go	2019-06-20 14:35:12.000000000 -0500
++++ b/storage/reads/group_resultset.go	2019-06-25 13:49:49.000000000 -0500
+@@ -7,7 +7,6 @@
+ 	"math"
+ 	"sort"
+ 
+-	"github.com/influxdata/influxdb/kit/tracing"
+ 	"github.com/influxdata/influxdb/models"
+ 	"github.com/influxdata/influxdb/storage/reads/datatypes"
+ 	"github.com/influxdata/influxdb/tsdb/cursors"
+@@ -112,16 +111,7 @@
+ }
+ 
+ func (g *groupResultSet) sort() (int, error) {
+-	span, _ := tracing.StartSpanFromContext(g.ctx)
+-	defer span.Finish()
+-	span.LogKV("group_type", g.req.Group.String())
+-
+ 	n, err := g.sortFn(g)
+-
+-	if err != nil {
+-		span.LogKV("rows", n)
+-	}
+-
+ 	return n, err
+ }
+ 
+diff -ur a/storage/reads/group_resultset_test.go b/storage/reads/group_resultset_test.go
+--- a/storage/reads/group_resultset_test.go	2019-06-20 14:35:12.000000000 -0500
++++ b/storage/reads/group_resultset_test.go	2019-06-25 13:49:49.000000000 -0500
+@@ -394,7 +394,7 @@
+ 		vals[i] = gen.NewCounterByteSequenceCount(card[i])
+ 	}
+ 
+-	tags := gen.NewTagsValuesSequenceValues("m0", "f0", "tag", vals)
++	tags := gen.NewTagsValuesSequenceValues("tag", vals)
+ 	rows := make([]reads.SeriesRow, tags.Count())
+ 	for i := range rows {
+ 		tags.Next()
+diff -ur a/storage/reads/reader.go b/storage/reads/reader.go
+--- a/storage/reads/reader.go	2019-06-20 14:35:12.000000000 -0500
++++ b/storage/reads/reader.go	2019-06-25 13:49:49.000000000 -0500
+@@ -10,8 +10,8 @@
+ 	"github.com/influxdata/flux/execute"
+ 	"github.com/influxdata/flux/memory"
+ 	"github.com/influxdata/flux/values"
++	"github.com/influxdata/influxdb/flux/stdlib/influxdata/influxdb"
+ 	"github.com/influxdata/influxdb/models"
+-	"github.com/influxdata/influxdb/query/stdlib/influxdata/influxdb"
+ 	"github.com/influxdata/influxdb/storage/reads/datatypes"
+ 	"github.com/influxdata/influxdb/tsdb/cursors"
+ )
+@@ -103,8 +103,8 @@
+ 
+ func (fi *filterIterator) Do(f func(flux.Table) error) error {
+ 	src := fi.s.GetSource(
+-		uint64(fi.spec.OrganizationID),
+-		uint64(fi.spec.BucketID),
++		fi.spec.Database,
++		fi.spec.RetentionPolicy,
+ 	)
+ 
+ 	// Setup read request
+@@ -225,8 +225,8 @@
+ 
+ func (gi *groupIterator) Do(f func(flux.Table) error) error {
+ 	src := gi.s.GetSource(
+-		uint64(gi.spec.OrganizationID),
+-		uint64(gi.spec.BucketID),
++		gi.spec.Database,
++		gi.spec.RetentionPolicy,
+ 	)
+ 
+ 	// Setup read request
+@@ -504,8 +504,8 @@
+ 
+ func (ti *tagKeysIterator) Do(f func(flux.Table) error) error {
+ 	src := ti.s.GetSource(
+-		uint64(ti.readSpec.OrganizationID),
+-		uint64(ti.readSpec.BucketID),
++		ti.readSpec.Database,
++		ti.readSpec.RetentionPolicy,
+ 	)
+ 
+ 	var req datatypes.TagKeysRequest
+@@ -586,8 +586,8 @@
+ 
+ func (ti *tagValuesIterator) Do(f func(flux.Table) error) error {
+ 	src := ti.s.GetSource(
+-		uint64(ti.readSpec.OrganizationID),
+-		uint64(ti.readSpec.BucketID),
++		ti.readSpec.Database,
++		ti.readSpec.RetentionPolicy,
+ 	)
+ 
+ 	var req datatypes.TagValuesRequest
+diff -ur a/storage/reads/store.go b/storage/reads/store.go
+--- a/storage/reads/store.go	2019-06-20 14:35:12.000000000 -0500
++++ b/storage/reads/store.go	2019-06-25 13:49:49.000000000 -0500
+@@ -80,5 +80,5 @@
+ 	TagKeys(ctx context.Context, req *datatypes.TagKeysRequest) (cursors.StringIterator, error)
+ 	TagValues(ctx context.Context, req *datatypes.TagValuesRequest) (cursors.StringIterator, error)
+ 
+-	GetSource(orgID, bucketID uint64) proto.Message
++	GetSource(db, rp string) proto.Message
+ }
+diff -ur a/storage/reads/table.go b/storage/reads/table.go
+--- a/storage/reads/table.go	2019-06-20 14:35:12.000000000 -0500
++++ b/storage/reads/table.go	2019-06-25 13:49:49.000000000 -0500
+@@ -1,7 +1,5 @@
+ package reads
+ 
+-//go:generate env GO111MODULE=on go run github.com/benbjohnson/tmpl -data=@types.tmpldata table.gen.go.tmpl
+-
+ import (
+ 	"sync/atomic"
+ 
+diff -ur a/tsdb/cursors/gen.go b/tsdb/cursors/gen.go
+--- a/tsdb/cursors/gen.go	2019-06-20 14:35:12.000000000 -0500
++++ b/tsdb/cursors/gen.go	2019-06-25 14:00:51.000000000 -0500
+@@ -1,3 +1 @@
+ package cursors
+-
+-//go:generate env GO111MODULE=on go run github.com/benbjohnson/tmpl -data=@arrayvalues.gen.go.tmpldata arrayvalues.gen.go.tmpl
diff --git b/storage/reads/group_resultset.go a/storage/reads/group_resultset.go
index c7ed990bb..5d6ca33a4 100644
--- b/storage/reads/group_resultset.go
+++ a/storage/reads/group_resultset.go
@@ -7,7 +7,6 @@ import (
 	"math"
 	"sort"
 
-	"github.com/influxdata/influxdb/kit/tracing"
 	"github.com/influxdata/influxdb/models"
 	"github.com/influxdata/influxdb/storage/reads/datatypes"
 	"github.com/influxdata/influxdb/tsdb/cursors"
@@ -112,16 +111,7 @@ func (g *groupResultSet) Next() GroupCursor {
 }
 
 func (g *groupResultSet) sort() (int, error) {
-	span, _ := tracing.StartSpanFromContext(g.ctx)
-	defer span.Finish()
-	span.LogKV("group_type", g.req.Group.String())
-
 	n, err := g.sortFn(g)
-
-	if err != nil {
-		span.LogKV("rows", n)
-	}
-
 	return n, err
 }
 
diff --git b/storage/reads/group_resultset_test.go a/storage/reads/group_resultset_test.go
index ee13d1616..eb1fc91fc 100644
--- b/storage/reads/group_resultset_test.go
+++ a/storage/reads/group_resultset_test.go
@@ -394,7 +394,7 @@ func BenchmarkNewGroupResultSet_GroupBy(b *testing.B) {
 		vals[i] = gen.NewCounterByteSequenceCount(card[i])
 	}
 
-	tags := gen.NewTagsValuesSequenceValues("m0", "f0", "tag", vals)
+	tags := gen.NewTagsValuesSequenceValues("tag", vals)
 	rows := make([]reads.SeriesRow, tags.Count())
 	for i := range rows {
 		tags.Next()
diff --git b/storage/reads/helpers_test.go a/storage/reads/helpers_test.go
index d688ae365..ff8698b89 100644
--- b/storage/reads/helpers_test.go
+++ a/storage/reads/helpers_test.go
@@ -1,169 +1 @@
 package reads_test
-
-import (
-	"context"
-
-	"github.com/influxdata/influxdb/models"
-	"github.com/influxdata/influxdb/pkg/data/gen"
-	"github.com/influxdata/influxdb/storage/reads"
-	"github.com/influxdata/influxdb/tsdb"
-	"github.com/influxdata/influxdb/tsdb/cursors"
-)
-
-type seriesGeneratorCursorIterator struct {
-	g   gen.SeriesGenerator
-	f   floatTimeValuesGeneratorCursor
-	i   integerTimeValuesGeneratorCursor
-	u   unsignedTimeValuesGeneratorCursor
-	s   stringTimeValuesGeneratorCursor
-	b   booleanTimeValuesGeneratorCursor
-	cur cursors.Cursor
-}
-
-func (ci *seriesGeneratorCursorIterator) Next(ctx context.Context, r *cursors.CursorRequest) (cursors.Cursor, error) {
-	switch ci.g.FieldType() {
-	case models.Float:
-		ci.f.tv = ci.g.TimeValuesGenerator()
-		ci.cur = &ci.f
-	case models.Integer:
-		ci.i.tv = ci.g.TimeValuesGenerator()
-		ci.cur = &ci.i
-	case models.Unsigned:
-		ci.u.tv = ci.g.TimeValuesGenerator()
-		ci.cur = &ci.u
-	case models.String:
-		ci.s.tv = ci.g.TimeValuesGenerator()
-		ci.cur = &ci.s
-	case models.Boolean:
-		ci.b.tv = ci.g.TimeValuesGenerator()
-		ci.cur = &ci.b
-	default:
-		panic("unreachable")
-	}
-
-	return ci.cur, nil
-}
-
-func (ci *seriesGeneratorCursorIterator) Stats() cursors.CursorStats {
-	return ci.cur.Stats()
-}
-
-type seriesGeneratorSeriesCursor struct {
-	ci seriesGeneratorCursorIterator
-	r  reads.SeriesRow
-}
-
-func newSeriesGeneratorSeriesCursor(g gen.SeriesGenerator) *seriesGeneratorSeriesCursor {
-	s := &seriesGeneratorSeriesCursor{}
-	s.ci.g = g
-	s.r.Query = tsdb.CursorIterators{&s.ci}
-	return s
-}
-
-func (s *seriesGeneratorSeriesCursor) Close()     {}
-func (s *seriesGeneratorSeriesCursor) Err() error { return nil }
-
-func (s *seriesGeneratorSeriesCursor) Next() *reads.SeriesRow {
-	if s.ci.g.Next() {
-		s.r.SeriesTags = s.ci.g.Tags()
-		s.r.Tags = s.ci.g.Tags()
-		return &s.r
-	}
-	return nil
-}
-
-type timeValuesGeneratorCursor struct {
-	tv    gen.TimeValuesSequence
-	stats cursors.CursorStats
-}
-
-func (t timeValuesGeneratorCursor) Close()                     {}
-func (t timeValuesGeneratorCursor) Err() error                 { return nil }
-func (t timeValuesGeneratorCursor) Stats() cursors.CursorStats { return t.stats }
-
-type floatTimeValuesGeneratorCursor struct {
-	timeValuesGeneratorCursor
-	a tsdb.FloatArray
-}
-
-func (c *floatTimeValuesGeneratorCursor) Next() *cursors.FloatArray {
-	if c.tv.Next() {
-		c.tv.Values().(gen.FloatValues).Copy(&c.a)
-	} else {
-		c.a.Timestamps = c.a.Timestamps[:0]
-		c.a.Values = c.a.Values[:0]
-	}
-	c.stats.ScannedBytes += len(c.a.Values) * 8
-	c.stats.ScannedValues += c.a.Len()
-	return &c.a
-}
-
-type integerTimeValuesGeneratorCursor struct {
-	timeValuesGeneratorCursor
-	a tsdb.IntegerArray
-}
-
-func (c *integerTimeValuesGeneratorCursor) Next() *cursors.IntegerArray {
-	if c.tv.Next() {
-		c.tv.Values().(gen.IntegerValues).Copy(&c.a)
-	} else {
-		c.a.Timestamps = c.a.Timestamps[:0]
-		c.a.Values = c.a.Values[:0]
-	}
-	c.stats.ScannedBytes += len(c.a.Values) * 8
-	c.stats.ScannedValues += c.a.Len()
-	return &c.a
-}
-
-type unsignedTimeValuesGeneratorCursor struct {
-	timeValuesGeneratorCursor
-	a tsdb.UnsignedArray
-}
-
-func (c *unsignedTimeValuesGeneratorCursor) Next() *cursors.UnsignedArray {
-	if c.tv.Next() {
-		c.tv.Values().(gen.UnsignedValues).Copy(&c.a)
-	} else {
-		c.a.Timestamps = c.a.Timestamps[:0]
-		c.a.Values = c.a.Values[:0]
-	}
-	c.stats.ScannedBytes += len(c.a.Values) * 8
-	c.stats.ScannedValues += c.a.Len()
-	return &c.a
-}
-
-type stringTimeValuesGeneratorCursor struct {
-	timeValuesGeneratorCursor
-	a tsdb.StringArray
-}
-
-func (c *stringTimeValuesGeneratorCursor) Next() *cursors.StringArray {
-	if c.tv.Next() {
-		c.tv.Values().(gen.StringValues).Copy(&c.a)
-	} else {
-		c.a.Timestamps = c.a.Timestamps[:0]
-		c.a.Values = c.a.Values[:0]
-	}
-	for _, v := range c.a.Values {
-		c.stats.ScannedBytes += len(v)
-	}
-	c.stats.ScannedValues += c.a.Len()
-	return &c.a
-}
-
-type booleanTimeValuesGeneratorCursor struct {
-	timeValuesGeneratorCursor
-	a tsdb.BooleanArray
-}
-
-func (c *booleanTimeValuesGeneratorCursor) Next() *cursors.BooleanArray {
-	if c.tv.Next() {
-		c.tv.Values().(gen.BooleanValues).Copy(&c.a)
-	} else {
-		c.a.Timestamps = c.a.Timestamps[:0]
-		c.a.Values = c.a.Values[:0]
-	}
-	c.stats.ScannedBytes += len(c.a.Values)
-	c.stats.ScannedValues += c.a.Len()
-	return &c.a
-}
diff --git b/storage/reads/reader.go a/storage/reads/reader.go
index f9765dbe1..38acd4157 100644
--- b/storage/reads/reader.go
+++ a/storage/reads/reader.go
@@ -10,8 +10,8 @@ import (
 	"github.com/influxdata/flux/execute"
 	"github.com/influxdata/flux/memory"
 	"github.com/influxdata/flux/values"
+	"github.com/influxdata/influxdb/flux/stdlib/influxdata/influxdb"
 	"github.com/influxdata/influxdb/models"
-	"github.com/influxdata/influxdb/query/stdlib/influxdata/influxdb"
 	"github.com/influxdata/influxdb/storage/reads/datatypes"
 	"github.com/influxdata/influxdb/tsdb/cursors"
 )
@@ -103,8 +103,8 @@ func (fi *filterIterator) Statistics() cursors.CursorStats { return fi.stats }
 
 func (fi *filterIterator) Do(f func(flux.Table) error) error {
 	src := fi.s.GetSource(
-		uint64(fi.spec.OrganizationID),
-		uint64(fi.spec.BucketID),
+		fi.spec.Database,
+		fi.spec.RetentionPolicy,
 	)
 
 	// Setup read request
@@ -225,8 +225,8 @@ func (gi *groupIterator) Statistics() cursors.CursorStats { return gi.stats }
 
 func (gi *groupIterator) Do(f func(flux.Table) error) error {
 	src := gi.s.GetSource(
-		uint64(gi.spec.OrganizationID),
-		uint64(gi.spec.BucketID),
+		gi.spec.Database,
+		gi.spec.RetentionPolicy,
 	)
 
 	// Setup read request
@@ -504,8 +504,8 @@ type tagKeysIterator struct {
 
 func (ti *tagKeysIterator) Do(f func(flux.Table) error) error {
 	src := ti.s.GetSource(
-		uint64(ti.readSpec.OrganizationID),
-		uint64(ti.readSpec.BucketID),
+		ti.readSpec.Database,
+		ti.readSpec.RetentionPolicy,
 	)
 
 	var req datatypes.TagKeysRequest
@@ -586,8 +586,8 @@ type tagValuesIterator struct {
 
 func (ti *tagValuesIterator) Do(f func(flux.Table) error) error {
 	src := ti.s.GetSource(
-		uint64(ti.readSpec.OrganizationID),
-		uint64(ti.readSpec.BucketID),
+		ti.readSpec.Database,
+		ti.readSpec.RetentionPolicy,
 	)
 
 	var req datatypes.TagValuesRequest
diff --git b/storage/reads/reader_test.go a/storage/reads/reader_test.go
index 4d660b3ff..8cc72783a 100644
--- b/storage/reads/reader_test.go
+++ a/storage/reads/reader_test.go
@@ -9,8 +9,8 @@ import (
 	"github.com/influxdata/flux/execute"
 	"github.com/influxdata/flux/execute/executetest"
 	"github.com/influxdata/flux/memory"
+	"github.com/influxdata/influxdb/flux/stdlib/influxdata/influxdb"
 	"github.com/influxdata/influxdb/mock"
-	"github.com/influxdata/influxdb/query/stdlib/influxdata/influxdb"
 	"github.com/influxdata/influxdb/storage/reads"
 	"github.com/influxdata/influxdb/storage/reads/datatypes"
 	"github.com/influxdata/influxdb/tsdb/cursors"
diff --git b/storage/reads/response_writer_test.go a/storage/reads/response_writer_test.go
index 0916c822b..0abaf7544 100644
--- b/storage/reads/response_writer_test.go
+++ a/storage/reads/response_writer_test.go
@@ -1,21 +1,12 @@
 package reads_test
 
 import (
-	"context"
-	"errors"
 	"fmt"
 	"reflect"
-	"strings"
 	"testing"
-	"time"
 
-	"github.com/influxdata/influxdb"
 	"github.com/influxdata/influxdb/mock"
-	"github.com/influxdata/influxdb/pkg/data/gen"
-	"github.com/influxdata/influxdb/pkg/testing/assert"
 	"github.com/influxdata/influxdb/storage/reads"
-	"github.com/influxdata/influxdb/storage/reads/datatypes"
-	"github.com/influxdata/influxdb/tsdb"
 	"github.com/influxdata/influxdb/tsdb/cursors"
 	"google.golang.org/grpc/metadata"
 )
@@ -132,403 +123,3 @@ func TestResponseWriter_WriteGroupResultSet_Stats(t *testing.T) {
 		t.Errorf("expected scanned-bytes '%v' but got '%v'", []string{fmt.Sprint(scannedBytes)}, gotTrailer.Get("scanned-bytes"))
 	}
 }
-
-var (
-	org         = influxdb.ID(0xff00ff00)
-	bucket      = influxdb.ID(0xcc00cc00)
-	orgBucketID = tsdb.EncodeName(org, bucket)
-)
-
-func makeTypedSeries(m, prefix, field string, val interface{}, valueCount int, counts ...int) gen.SeriesGenerator {
-	spec := gen.TimeSequenceSpec{Count: valueCount, Start: time.Unix(0, 0), Delta: time.Second}
-	ts := gen.NewTimestampSequenceFromSpec(spec)
-	var vg gen.TimeValuesSequence
-	switch val := val.(type) {
-	case float64:
-		vg = gen.NewTimeFloatValuesSequence(spec.Count, ts, gen.NewFloatConstantValuesSequence(val))
-	case int64:
-		vg = gen.NewTimeIntegerValuesSequence(spec.Count, ts, gen.NewIntegerConstantValuesSequence(val))
-	case int:
-		vg = gen.NewTimeIntegerValuesSequence(spec.Count, ts, gen.NewIntegerConstantValuesSequence(int64(val)))
-	case uint64:
-		vg = gen.NewTimeUnsignedValuesSequence(spec.Count, ts, gen.NewUnsignedConstantValuesSequence(val))
-	case string:
-		vg = gen.NewTimeStringValuesSequence(spec.Count, ts, gen.NewStringConstantValuesSequence(val))
-	case bool:
-		vg = gen.NewTimeBooleanValuesSequence(spec.Count, ts, gen.NewBooleanConstantValuesSequence(val))
-	default:
-		panic(fmt.Sprintf("unexpected type %T", val))
-	}
-
-	return gen.NewSeriesGenerator(orgBucketID, []byte(field), vg, gen.NewTagsValuesSequenceCounts(m, field, prefix, counts))
-}
-
-type sendSummary struct {
-	groupCount    int
-	seriesCount   int
-	floatCount    int
-	integerCount  int
-	unsignedCount int
-	stringCount   int
-	booleanCount  int
-}
-
-func (ss *sendSummary) makeSendFunc() func(*datatypes.ReadResponse) error {
-	return func(r *datatypes.ReadResponse) error {
-		for i := range r.Frames {
-			d := r.Frames[i].Data
-			switch p := d.(type) {
-			case *datatypes.ReadResponse_Frame_FloatPoints:
-				ss.floatCount += len(p.FloatPoints.Values)
-			case *datatypes.ReadResponse_Frame_IntegerPoints:
-				ss.integerCount += len(p.IntegerPoints.Values)
-			case *datatypes.ReadResponse_Frame_UnsignedPoints:
-				ss.unsignedCount += len(p.UnsignedPoints.Values)
-			case *datatypes.ReadResponse_Frame_StringPoints:
-				ss.stringCount += len(p.StringPoints.Values)
-			case *datatypes.ReadResponse_Frame_BooleanPoints:
-				ss.booleanCount += len(p.BooleanPoints.Values)
-			case *datatypes.ReadResponse_Frame_Series:
-				ss.seriesCount++
-			case *datatypes.ReadResponse_Frame_Group:
-				ss.groupCount++
-			default:
-				panic("unexpected")
-			}
-		}
-		return nil
-	}
-}
-
-func TestResponseWriter_WriteResultSet(t *testing.T) {
-	t.Run("normal", func(t *testing.T) {
-		t.Run("all types one series each", func(t *testing.T) {
-			exp := sendSummary{
-				seriesCount:   5,
-				floatCount:    500,
-				integerCount:  400,
-				unsignedCount: 300,
-				stringCount:   200,
-				booleanCount:  100,
-			}
-			var ss sendSummary
-
-			stream := mock.NewResponseStream()
-			stream.SendFunc = ss.makeSendFunc()
-			w := reads.NewResponseWriter(stream, 0)
-
-			var gens []gen.SeriesGenerator
-
-			gens = append(gens, makeTypedSeries("m0", "t", "ff", 3.3, exp.floatCount, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "if", 100, exp.integerCount, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "uf", uint64(25), exp.unsignedCount, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "sf", "foo", exp.stringCount, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "bf", false, exp.booleanCount, 1))
-
-			cur := newSeriesGeneratorSeriesCursor(gen.NewMergedSeriesGenerator(gens))
-			rs := reads.NewFilteredResultSet(context.Background(), &datatypes.ReadFilterRequest{}, cur)
-			err := w.WriteResultSet(rs)
-			if err != nil {
-				t.Fatalf("unexpected err: %v", err)
-			}
-			w.Flush()
-
-			assert.Equal(t, ss, exp)
-		})
-		t.Run("multi-series floats", func(t *testing.T) {
-			exp := sendSummary{
-				seriesCount: 5,
-				floatCount:  8600,
-			}
-			var ss sendSummary
-
-			stream := mock.NewResponseStream()
-			stream.SendFunc = ss.makeSendFunc()
-			w := reads.NewResponseWriter(stream, 0)
-
-			var gens []gen.SeriesGenerator
-			gens = append(gens, makeTypedSeries("m0", "t", "f0", 3.3, 2000, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "f1", 5.3, 1500, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "f2", 5.3, 2500, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "f3", -2.2, 900, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "f4", -9.2, 1700, 1))
-
-			cur := newSeriesGeneratorSeriesCursor(gen.NewMergedSeriesGenerator(gens))
-			rs := reads.NewFilteredResultSet(context.Background(), &datatypes.ReadFilterRequest{}, cur)
-			err := w.WriteResultSet(rs)
-			if err != nil {
-				t.Fatalf("unexpected err: %v", err)
-			}
-			w.Flush()
-
-			assert.Equal(t, ss, exp)
-		})
-
-		t.Run("multi-series strings", func(t *testing.T) {
-			exp := sendSummary{
-				seriesCount: 4,
-				stringCount: 6900,
-			}
-			var ss sendSummary
-
-			stream := mock.NewResponseStream()
-			stream.SendFunc = ss.makeSendFunc()
-			w := reads.NewResponseWriter(stream, 0)
-
-			var gens []gen.SeriesGenerator
-			gens = append(gens, makeTypedSeries("m0", "t", "s0", strings.Repeat("aaa", 100), 2000, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "s1", strings.Repeat("bbb", 200), 1500, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "s2", strings.Repeat("ccc", 300), 2500, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "s3", strings.Repeat("ddd", 200), 900, 1))
-
-			cur := newSeriesGeneratorSeriesCursor(gen.NewMergedSeriesGenerator(gens))
-			rs := reads.NewFilteredResultSet(context.Background(), &datatypes.ReadFilterRequest{}, cur)
-			err := w.WriteResultSet(rs)
-			if err != nil {
-				t.Fatalf("unexpected err: %v", err)
-			}
-			w.Flush()
-
-			assert.Equal(t, ss, exp)
-		})
-
-		t.Run("writer doesn't send series with no values", func(t *testing.T) {
-			exp := sendSummary{
-				seriesCount: 2,
-				stringCount: 3700,
-			}
-			var ss sendSummary
-
-			stream := mock.NewResponseStream()
-			stream.SendFunc = ss.makeSendFunc()
-			w := reads.NewResponseWriter(stream, 0)
-
-			var gens []gen.SeriesGenerator
-			gens = append(gens, makeTypedSeries("m0", "t", "s0", strings.Repeat("aaa", 100), 2000, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "s1", strings.Repeat("bbb", 200), 0, 1))
-			gens = append(gens, makeTypedSeries("m0", "t", "s2", strings.Repeat("ccc", 100), 1700, 1))
-			cur := newSeriesGeneratorSeriesCursor(gen.NewMergedSeriesGenerator(gens))
-
-			rs := reads.NewFilteredResultSet(context.Background(), &datatypes.ReadFilterRequest{}, cur)
-			err := w.WriteResultSet(rs)
-			if err != nil {
-				t.Fatalf("unexpected err: %v", err)
-			}
-			w.Flush()
-
-			assert.Equal(t, ss, exp)
-		})
-	})
-
-	t.Run("error conditions", func(t *testing.T) {
-		t.Run("writer returns stream error", func(t *testing.T) {
-			exp := errors.New("no write")
-
-			stream := mock.NewResponseStream()
-			stream.SendFunc = func(r *datatypes.ReadResponse) error { return exp }
-			w := reads.NewResponseWriter(stream, 0)
-
-			cur := newSeriesGeneratorSeriesCursor(makeTypedSeries("m0", "t", "f0", strings.Repeat("0", 1000), 2000, 1))
-			rs := reads.NewFilteredResultSet(context.Background(), &datatypes.ReadFilterRequest{}, cur)
-			_ = w.WriteResultSet(rs)
-			assert.Equal(t, w.Err(), exp)
-		})
-	})
-
-	t.Run("issues", func(t *testing.T) {
-		t.Run("short write", func(t *testing.T) {
-			t.Run("single string series", func(t *testing.T) {
-				exp := sendSummary{seriesCount: 1, stringCount: 1020}
-				var ss sendSummary
-
-				stream := mock.NewResponseStream()
-				stream.SendFunc = ss.makeSendFunc()
-				w := reads.NewResponseWriter(stream, 0)
-
-				cur := newSeriesGeneratorSeriesCursor(makeTypedSeries("m0", "t", "f0", strings.Repeat("0", 1000), exp.stringCount, 1))
-				rs := reads.NewFilteredResultSet(context.Background(), &datatypes.ReadFilterRequest{}, cur)
-				err := w.WriteResultSet(rs)
-				if err != nil {
-					t.Fatalf("unexpected err: %v", err)
-				}
-				w.Flush()
-
-				assert.Equal(t, ss, exp)
-			})
-
-			t.Run("single float series", func(t *testing.T) {
-				exp := sendSummary{seriesCount: 1, floatCount: 50500}
-				var ss sendSummary
-
-				stream := mock.NewResponseStream()
-				stream.SendFunc = ss.makeSendFunc()
-				w := reads.NewResponseWriter(stream, 0)
-
-				cur := newSeriesGeneratorSeriesCursor(makeTypedSeries("m0", "t", "f0", 5.5, exp.floatCount, 1))
-				rs := reads.NewFilteredResultSet(context.Background(), &datatypes.ReadFilterRequest{}, cur)
-				err := w.WriteResultSet(rs)
-				if err != nil {
-					t.Fatalf("unexpected err: %v", err)
-				}
-				w.Flush()
-
-				assert.Equal(t, ss, exp)
-			})
-
-			t.Run("multi series", func(t *testing.T) {
-				exp := sendSummary{seriesCount: 2, stringCount: 3700}
-				var ss sendSummary
-
-				stream := mock.NewResponseStream()
-				stream.SendFunc = ss.makeSendFunc()
-				w := reads.NewResponseWriter(stream, 0)
-
-				var gens []gen.SeriesGenerator
-				gens = append(gens, makeTypedSeries("m0", "t", "s0", strings.Repeat("aaa", 1000), 2200, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "s1", strings.Repeat("bbb", 1000), 1500, 1))
-
-				cur := newSeriesGeneratorSeriesCursor(gen.NewMergedSeriesGenerator(gens))
-				rs := reads.NewFilteredResultSet(context.Background(), &datatypes.ReadFilterRequest{}, cur)
-				err := w.WriteResultSet(rs)
-				if err != nil {
-					t.Fatalf("unexpected err: %v", err)
-				}
-				w.Flush()
-
-				assert.Equal(t, ss, exp)
-			})
-		})
-	})
-}
-
-func TestResponseWriter_WriteGroupResultSet(t *testing.T) {
-	t.Run("normal", func(t *testing.T) {
-		t.Run("all types one series each", func(t *testing.T) {
-			exp := sendSummary{
-				groupCount:    1,
-				seriesCount:   5,
-				floatCount:    500,
-				integerCount:  400,
-				unsignedCount: 300,
-				stringCount:   200,
-				booleanCount:  100,
-			}
-			var ss sendSummary
-
-			stream := mock.NewResponseStream()
-			stream.SendFunc = ss.makeSendFunc()
-			w := reads.NewResponseWriter(stream, 0)
-
-			newCursor := func() (cursor reads.SeriesCursor, e error) {
-				var gens []gen.SeriesGenerator
-				gens = append(gens, makeTypedSeries("m0", "t", "ff", 3.3, exp.floatCount, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "if", 100, exp.integerCount, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "uf", uint64(25), exp.unsignedCount, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "sf", "foo", exp.stringCount, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "bf", false, exp.booleanCount, 1))
-				return newSeriesGeneratorSeriesCursor(gen.NewMergedSeriesGenerator(gens)), nil
-			}
-
-			rs := reads.NewGroupResultSet(context.Background(), &datatypes.ReadGroupRequest{Group: datatypes.GroupNone}, newCursor)
-			err := w.WriteGroupResultSet(rs)
-			if err != nil {
-				t.Fatalf("unexpected err: %v", err)
-			}
-			w.Flush()
-
-			assert.Equal(t, ss, exp)
-		})
-		t.Run("multi-series floats", func(t *testing.T) {
-			exp := sendSummary{
-				groupCount:  1,
-				seriesCount: 5,
-				floatCount:  8600,
-			}
-			var ss sendSummary
-
-			stream := mock.NewResponseStream()
-			stream.SendFunc = ss.makeSendFunc()
-			w := reads.NewResponseWriter(stream, 0)
-
-			newCursor := func() (cursor reads.SeriesCursor, e error) {
-				var gens []gen.SeriesGenerator
-				gens = append(gens, makeTypedSeries("m0", "t", "f0", 3.3, 2000, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "f1", 5.3, 1500, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "f2", 5.3, 2500, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "f3", -2.2, 900, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "f4", -9.2, 1700, 1))
-				return newSeriesGeneratorSeriesCursor(gen.NewMergedSeriesGenerator(gens)), nil
-			}
-
-			rs := reads.NewGroupResultSet(context.Background(), &datatypes.ReadGroupRequest{Group: datatypes.GroupNone}, newCursor)
-			err := w.WriteGroupResultSet(rs)
-			if err != nil {
-				t.Fatalf("unexpected err: %v", err)
-			}
-			w.Flush()
-
-			assert.Equal(t, ss, exp)
-		})
-
-		t.Run("multi-series strings", func(t *testing.T) {
-			exp := sendSummary{
-				groupCount:  1,
-				seriesCount: 4,
-				stringCount: 6900,
-			}
-			var ss sendSummary
-
-			stream := mock.NewResponseStream()
-			stream.SendFunc = ss.makeSendFunc()
-			w := reads.NewResponseWriter(stream, 0)
-
-			newCursor := func() (cursor reads.SeriesCursor, e error) {
-				var gens []gen.SeriesGenerator
-				gens = append(gens, makeTypedSeries("m0", "t", "s0", strings.Repeat("aaa", 100), 2000, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "s1", strings.Repeat("bbb", 200), 1500, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "s2", strings.Repeat("ccc", 300), 2500, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "s3", strings.Repeat("ddd", 200), 900, 1))
-				return newSeriesGeneratorSeriesCursor(gen.NewMergedSeriesGenerator(gens)), nil
-			}
-
-			rs := reads.NewGroupResultSet(context.Background(), &datatypes.ReadGroupRequest{Group: datatypes.GroupNone}, newCursor)
-			err := w.WriteGroupResultSet(rs)
-			if err != nil {
-				t.Fatalf("unexpected err: %v", err)
-			}
-			w.Flush()
-
-			assert.Equal(t, ss, exp)
-		})
-
-		t.Run("writer doesn't send series with no values", func(t *testing.T) {
-			exp := sendSummary{
-				groupCount:  1,
-				seriesCount: 2,
-				stringCount: 3700,
-			}
-			var ss sendSummary
-
-			stream := mock.NewResponseStream()
-			stream.SendFunc = ss.makeSendFunc()
-			w := reads.NewResponseWriter(stream, 0)
-
-			newCursor := func() (cursor reads.SeriesCursor, e error) {
-				var gens []gen.SeriesGenerator
-				gens = append(gens, makeTypedSeries("m0", "t", "s0", strings.Repeat("aaa", 100), 2000, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "s1", strings.Repeat("bbb", 200), 0, 1))
-				gens = append(gens, makeTypedSeries("m0", "t", "s2", strings.Repeat("ccc", 100), 1700, 1))
-				return newSeriesGeneratorSeriesCursor(gen.NewMergedSeriesGenerator(gens)), nil
-			}
-
-			rs := reads.NewGroupResultSet(context.Background(), &datatypes.ReadGroupRequest{Group: datatypes.GroupNone}, newCursor)
-			err := w.WriteGroupResultSet(rs)
-			if err != nil {
-				t.Fatalf("unexpected err: %v", err)
-			}
-			w.Flush()
-
-			assert.Equal(t, ss, exp)
-		})
-	})
-}
diff --git b/storage/reads/store.go a/storage/reads/store.go
index 8918794b3..655d12d21 100644
--- b/storage/reads/store.go
+++ a/storage/reads/store.go
@@ -80,5 +80,5 @@ type Store interface {
 	TagKeys(ctx context.Context, req *datatypes.TagKeysRequest) (cursors.StringIterator, error)
 	TagValues(ctx context.Context, req *datatypes.TagValuesRequest) (cursors.StringIterator, error)
 
-	GetSource(orgID, bucketID uint64) proto.Message
+	GetSource(db, rp string) proto.Message
 }
diff --git b/storage/reads/table.go a/storage/reads/table.go
index 32784a538..3aa7485e2 100644
--- b/storage/reads/table.go
+++ a/storage/reads/table.go
@@ -1,7 +1,5 @@
 package reads
 
-//go:generate env GO111MODULE=on go run github.com/benbjohnson/tmpl -data=@types.tmpldata table.gen.go.tmpl
-
 import (
 	"errors"
 	"sync/atomic"
diff --git b/tsdb/cursors/gen.go a/tsdb/cursors/gen.go
index 63316e5c0..ee7a8876a 100644
--- b/tsdb/cursors/gen.go
+++ a/tsdb/cursors/gen.go
@@ -1,3 +1 @@
 package cursors
-
-//go:generate env GO111MODULE=on go run github.com/benbjohnson/tmpl -data=@arrayvalues.gen.go.tmpldata arrayvalues.gen.go.tmpl
